{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Blog","text":""},{"location":"about-me/","title":"About me","text":"<p>I\u2019ve been an actuary all my working life. It wasn\u2019t my original plan, but having run out of options at the end of my university degree, I assumed my second year room-mate had done his research and followed him into the actuarial profession.</p> <p>The work was fairly mundane until at Bacon &amp; Woodrow (now Aon) I came into contact with Andrew Smith in 1998 and then Mercer\u2019s Jon Exley, from whom I learned that important aspects of finance had either been omitted from the actuarial syllabus or distorted in their presentation by \u2013 what seemed to me at the time to be \u2013 a reactionary profession. In an effort to address this, I spoke at sessional meetings, published papers and even ended up getting elected to the Council of the Institute of Actuaries. I was in excellent company with Andrew, Jon, Cliff Speed, Charles Cowling and the evergreen John Ralfe (who has summarised those times here).</p> <p>For about a year, I was seconded to our investment team \u2013 I assumed to help bring some of these ideas into practice \u2013 but what was emerging, namely \u2018LDI\u2019, was a long way away from what we had advocated.</p> <p>I then did a stint of corporate actuarial consulting (before it was called that) and, given what I perceived to be a relatively complacent attitude to pension plan deficits, raised awareness of the importance of monitoring the pension plan sponsors\u2019 own financial condition.</p> <p>After a short break, I spent the 15 years to the end of 2023 as head of what is now Demographic Horizons at Aon, focussing on forecasting pensioner longevity for defined benefit pension plans. Along the way, I advised on the largest longevity transactions in the UK, the US and continental Europe and chaired the UK\u2019s Continuous Mortality Investigation and, in particular, its Mortality Projections Committee. (If you\u2019re interested in a little more detail then see this LinkedIn post.)</p> <p>I\u2019m currently working on some things that I\u2019ve wanted to do for a long time but not had the chance.</p> <p>If you want to get in contact then please DM me on LinkedIn.</p>"},{"location":"collated-mortality-insights/","title":"Collated mortality insights","text":"<p>These are the collated mortality insights from all my blog articles.</p> <p>Insight 1. Always allow for overdispersion</p> <p>If you don\u2019t allow for overdispersion then you will underestimate uncertainty and overfit models.</p> <p>[Original article]</p> <p>Insight 2. Experience data is \u2018measurable\u2018</p> <p>Provided we use measures, we\u2019ll always get the same answer regardless of how an experience dataset is partitioned.</p> <p>In particular, there is no need</p> <ul> <li>for experience time periods to be contiguous<sup>1</sup> \u2013 the sole requirement is that elements of the experience datasets do not intersect, or</li> <li>to track individuals across experience datasets relating to different time periods<sup>2</sup>.</li> </ul> <p>[Original article]</p> <p>Insight 3. The continuous time definitions of A and E are canonical</p> <p>The continuous time definitions of \\(A\\) and \\(E\\) are measures and the canonical definitions of actual and expected deaths.</p> <p>Other definitions can lead to confusion \u2013 usually over \\(\\text{E}\\) vs true expectation \u2013 and spurious complexity.</p> <p>[Original article]</p> <p>Insight 4. The expected value of A\u2212E is zero</p> <p>If \\(\\mu\\) is the true mortality then the expected value of \\(\\text{A}f-\\text{E}f\\) is zero, i.e.</p> <pre>\\[\\mathbb{E}\\big(\\text{A}f-\\text{E}f\\big)=0\\]</pre> <ul> <li>for any variable \\(f\\) (even if \\(f\\) was used to fit the mortality in question), and</li> <li>for any subset of the experience data (provided the choice of subset does not depend on E2R information).</li> </ul> <p>[Original article]</p> <p>Insight 5. The same machinery that defines A\u2212E can be used to estimate its uncertainty</p> <p>If \\(\\mu\\) is the true mortality then, before allowing for overdispersion, the variance of \\(\\text{A}f-\\text{E}f\\) equals the expected value of \\(\\text{E}f^2\\), i.e.</p> <pre>\\[\\text{Var}\\big(\\text{A}f-\\text{E}f\\big)=\\mathbb{E}\\big(\\text{E}f^2\\big)\\]</pre> <p>Allowing for overdispersion \\(\\Omega\\), this becomes</p> <pre>\\[\\text{Var}\\big(\\text{A}f-\\text{E}f\\big)=\\Omega\\,\\mathbb{E}\\big(\\text{E}f^2\\big)\\]</pre> <p>Caveat: \\(f\\) is an ad hoc reallocation of log-likelihood; it is not relevance. For the version of this insight that does take account of relevance, see Insight\u00a017.</p> <p>[Original article]</p> <p>Insight 6. A/E variance increases with concentration</p> <p>\\(\\sqrt{\\text{E}w^2} / \\text{E}w\\), where \\(w\\ge0\\) is a useful and recurring measure of effective concentration in relation to mortality uncertainty. It implies that the more concentrated the experience data (in some sense) then the greater the variance of observed mortality.</p> <p>Using unweighted variance without adjustment to estimate weighted statistics will likely understate risk.</p> <p>[Original article]</p> <p>Insight 7. Log-likelihood can be defined directly in terms of the \\(\\text{A}\\) and \\(\\text{E}\\) operators</p> <p>The log-likelihood written in terms of the \\(\\text{A}\\) and \\(\\text{E}\\) operators is</p> <pre>\\[L=\\text{A}w\\log\\mu-\\text{E}w\\]</pre> <p>where \\(w\\ge0\\) is the weight variable.</p> <p>(This is before allowing for overdispersion.)</p> <p>[Original article]</p> <p>Insight 8. Proportional hazards models are probably all you need for mortality modelling</p> <p>The proportional hazards model</p> <pre>\\[\\mu(\\beta) = \\mu^\\text{ref}\\exp\\Big(\\beta^\\text{T}X\\Big)\\]</pre> <p>is</p> <ul> <li>highly tractable, and</li> <li>sufficiently powerful to cope with almost all practical mortality modelling problems.</li> </ul> <p>[Original article]</p> <p>Insight 9. An estimate of the variance of the fitted parameters for a proportional hazards mortality model is available in closed form for any ad hoc log-likelihood weight</p> <pre>\\[\\text{Var}\\big(\\hat\\beta\\big)\\mathrel{\\hat=} \\Omega\\,\\mathbf{I}^{-1}\\mathbf{J}\\mathbf{I}^{-1}\\]</pre> <p>where \\(\\hat\\beta\\) is the maximum likelihood estimator of the covariate weights, \\(X\\) is the vector of covariates, \\(w\\ge0\\) is the log-likelihood weight, \\(\\mathbf{I}=\\text{E}wXX^\\text{T}\\), \\(\\mathbf{J}=\\text{E}w^2XX^\\text{T}\\) and \\(\\Omega\\) is overdispersion</p> <p>Caveat: \\(w\\) is an ad hoc reallocation of log-likelihood; it is not relevance. For the version of this insight that does take account of relevance, see Insight\u00a017.</p> <p>[Original article]</p> <p>Insight 10. A penalised log-likelihood for a proportional hazards mortality model is available in closed form for any ad hoc log-likelihood weight</p> <pre>\\[L_\\text{P}= L(\\hat\\beta)-\\text{tr}\\big(\\mathbf{J}\\mathbf{I}^{-1}\\big)\\]</pre> <p>where \\(\\hat\\beta\\) is the maximum likelihood estimator of the covariate weights, \\(X\\) is the vector of covariates, \\(L\\) is the log-likelihood (which has already been adjusted for overdispersion), \\(w\\ge0\\) is the log-likelihood weight, \\(\\mathbf{I}=\\text{E}wXX^\\text{T}\\) and \\(\\mathbf{J}=\\text{E}w^2XX^\\text{T}\\).</p> <p>Caveat: \\(w\\) is an ad hoc reallocation of log-likelihood; it is not relevance. For the version of this insight that does take account of relevance, see Insight\u00a017.</p> <p>[Original article]</p> <p>Insight 11. Adjusting globally for overdispersion is reasonable and straightforward</p> <p>If \\(\\Omega\\) is global overdispersion then:</p> <ol> <li> <p>A standard method for allowing for overdispersion is to scale log-likelihood by \\(\\Omega^{-1}\\) and variances by \\(\\Omega\\).</p> </li> <li> <p>Suitable default values for mortality experience data are \\(2\\le\\Omega\\le3\\).</p> </li> <li> <p>Use the same \\(\\Omega\\) for all candidate models being tested, including when \\(\\Omega\\) is being estimated from the experience data at hand.</p> </li> </ol> <p>[Original article]</p> <p>Insight 12. Rating factors must be coherent</p> <p>In order for a function of information associated with individuals to be valid as a rating factor, it must be coherent, which means:</p> <ol> <li>No foreknowledge of death</li> <li>Correspondence between exits and survivors</li> <li>Comparability between individuals</li> <li>Comparability by time</li> </ol> <p>[Original article]</p> <p>Insight 13. Take care when using pension as a rating factor</p> <p>Be wary of phrases like \u2018just use pension as a covariate\u2019 because it trivialises the problems involved in making pension a coherent rating factor:</p> <ul> <li>Pensions for individuals in different pension plans are not directly comparable. For general pension plan mortality models consider using leave-one out cross validation to understand this risk and/or using an alternative approach.</li> <li>Pensions as at date of exit need careful adjustment to be consistent with pensions of survivors (which can be non trivial for UK DB plans).</li> <li>Pensions for actives require additional consideration in relation to potential future accrual.</li> <li>Consideration needs to be given to whether or how to adjust pensions for inflation (typically since retirement). This is more of an issue in pension systems where indexation of pensions in payment is less common (e.g. the USA).</li> <li>Do not assume that longevity always increases with benefit amount.</li> </ul> <p>[Original article]</p> <p>Insight 14. The bulk of pension plan mortality variation can be captured on a monotonic one dimensional scale</p> <p>Modelling base mortality for UK DB pension plans can be reasonably reduced to modelling a single parameter for each of male pensioners, female retirees and female dependants, i.e.</p> <pre>\\[\\mu_{it}(\\beta)= \\mu_{it}^\\text{ref} \\exp\\big(\\beta\\psi_x\\big)\\]</pre> <p>where \\(x\\) is age as a function of birth date from individual data \\(i\\) and time \\(t\\), \\(\\mu_{it}^\\text{ref}\\) is a common base mortality and \\(\\psi_x\\) is a common (non-negative) pattern of mortality variation by age that tends to zero at high ages.</p> <p>[Original article]</p> <p>Insight 15. Weighted log-likelihood automatically estimates liabilities correctly for single scalar parameter models when provided with relevance</p> <p>If (a)\u00a0a mortality model has a single scalar parameter and (b)\u00a0relevance is provided then maximising log-likelihood weighted by</p> <pre>\\[w_{it}=\\sum_{j\\in\\text{Val}} r_{it}^{jt_0} \\, I_j^{-1} v'_j\\]</pre> <p>automatically results in the best estimate of the present value of liabilities.</p> <p>In the above,</p> <ul> <li>\\(r_{it}^{jt_0}\\) is the relevance of the log-likelihood of the E2R of individual \\(i\\) at time \\(t\\) to individual \\(k\\) in the valuation data as at the valuation date \\(t_0\\),</li> <li>\\(I_j\\) is the relevant information matrix for valuation individual \\(j\\), and</li> <li>\\(v'_j\\) is derivative of liability value for valuation individual \\(j\\) with respect to the model parameter \\(\\beta\\).</li> </ul> <p>For further definitions, see article body.</p> <p>[Original article]</p> <p>Insight 16. A different weight is required to determine uncertainty in the presence of relevance</p> <p>The log-likelihood weight to determine uncertainty that corresponds to the best estimate weight in Insight\u00a015 is</p> <pre>\\[u_{it}=\\sum_{j\\in\\text{Val}} \\sqrt{r_{it}^{jt_0}} \\, I_j^{-1} v'_j\\]</pre> <p>[Original article]</p> <p>Insight 17. Always allow for time-based relevance</p> <p>Allowing for time-based relevance, e.g. using </p> <pre>\\[r_s^t=\\exp\\!\\big(\\!-\\phi\\,\\big|t-s\\big|\\big)\\]</pre> <p>where \\(s\\) and \\(t\\) are dates (measured in years) and \\(\\phi&gt;0\\), is to be preferred in all mortality modelling contexts because</p> <ul> <li>it automatically allows for the decay in relevance as time elapses, and</li> <li>compared with fixed windows, leaves models less sensitive to the falling away of more historical data.</li> </ul> <p>If relevance is purely time-based then this can be accomplished simply by scaling the experience data.</p> <p>[Original article]</p> <p>Insight 18. Use relevance for calibrating and selecting DB pensioner base mortality models</p> <p>Using the weights by \\(w\\) and \\(u\\) as defined in Insights\u00a015 and 16 respectively to calibrate and select DB pensioner base mortality models </p> <ul> <li> <p>takes explicit account of liability impact, and</p> </li> <li> <p>defaults to sensible results regardless of the quantum of experience data available.</p> </li> </ul> <p>The following Insights need to be restated to accommodate relevance:</p> <ul> <li> <p>Insight\u00a05 (allowing for overdispersion \\(\\Omega\\)) becomes</p> <pre>\\[\\text{Var}\\big(\\text{A}w-\\text{E}w\\big)=\\Omega\\,\\mathbb{E}\\big(\\text{E}u^2\\big)\\]</pre> </li> <li> <p>The equations for Insights\u00a09 and Insights\u00a010 are unchanged as</p> <pre>\\[\\begin{aligned}\n\\text{Var}\\big(\\hat\\beta\\big)\\mathrel{\\hat=} \\Omega\\,\\mathbf{I}^{-1}\\mathbf{J}\\mathbf{I}^{-1}\n\\\\[1em]\nL_\\text{P}= L(\\hat\\beta)-\\text{tr}\\big(\\mathbf{J}\\mathbf{I}^{-1}\\big)\n\\end{aligned}\\]</pre> <p>But \\(\\mathbf{J}\\) is redefined as \\(\\text{E}u^2XX^\\text{T}\\), i.e. weighted by \\(u^2\\) rather than \\(w^2\\).</p> </li> </ul> <p>[Original article]</p> <p>Insight 19. Prefer amounts-weighted to lives-weighted log-likelihood</p> <p>For DB pensioner mortality analysis, prefer statistics weighted by pension amount over lives-weighted.</p> <p>When there is a lot of experience data it won\u2019t matter; when there is only a little it will mitigate biased liability value estimates.</p> <p>[Original article]</p> <ol> <li> <p>An obvious example is excluding mortality experience from the height of the COVID-19 pandemic, potentially resulting in non-contiguous data from before and after the excluded time period.\u00a0\u21a9</p> </li> <li> <p>Tracking individuals across experience datasets for different time periods may however be a very sensible data check.\u00a0\u21a9</p> </li> </ol>"},{"location":"reprint-policy/","title":"Reprint policy","text":"<p>Feel free to republish items on this site provided you follow these guidelines:</p> <ol> <li> <p>Please</p> <ul> <li>attribute me as the author, and</li> <li>include a link to the original on this site.</li> </ul> </li> <li> <p>Please do not maliciously construct an excerpt of a posting here that means something different when taken out of context.</p> </li> <li> <p>If you republish something and someone sues you, it\u2019s your liability, not mine.</p> </li> </ol> <p>It would be nice if you let me know what articles you are republishing and where, but I\u2019ll leave that to you.</p> <p>Cheers</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"2024-01/lisnoti-a-proportional-font-that-works-for-coding-too/","title":"Lisnoti \u2013 a proportional font that works for coding too","text":"<p>Over the past 15 years or so, I have periodically searched in vain for a proportional sans serif font that is both high quality in itself but also suitable for programming, i.e. writing computer code. I finally decided to stop waiting and to make one myself.</p> <p>The result is the open source Lisnoti (<code>/l\u026az\u02c8n\u0259\u028ati\u02d0/</code>) font, which can be downloaded here. It is also the font used to typeset this blog.</p>"},{"location":"2024-01/lisnoti-a-proportional-font-that-works-for-coding-too/#the-most-extreme-typewriter-habit-of-them-all","title":"The most extreme typewriter habit of them all","text":"<p>It\u2019s a curious thing that almost no-one other than coders uses monospaced fonts for large amounts of text. Proportional fonts are ubiquitous for the simple reason that characters such as <code>W</code> and <code>i</code> have very different natural widths. Trying to squash <code>W</code> and widen <code>i</code> so that they have same width is at best inefficient and at worst less readable. And this is before we consider non-Western languages that use characters that are wider still.</p> <p>The convention that monospaced fonts are used for coding derives from programming\u2019s historical reliance on (a)\u00a0typewriter-like terminals and displays, and (b)\u00a0the ASCII character set (or similar), which can just about be crammed into a fixed width font.</p> <p>Typewriter habits</p> <p>\u2018Typewriter habits\u2019 is a term coined by Matthew Butterick in his excellent Practical Typography to describe bad typography habits left over from the typewriter era.</p> <p>The case for using proportional fonts for coding \u2013 essentially readability and efficiency \u2013 has already been well made by Mart\u0133n Storck, Nelson Minar and David Jonathan Ross. But the sheer prevalence of monospaced fonts in coding contexts means that their use is often just taken for granted and so the habit is hard to break, despite widespread IDE support for proportional fonts and despite computing having spread far beyond the English-speaking world (hello Unicode).</p> <p>Sometimes monospace is precisely what you want (e.g. to edit hex data, write assembly or use a terminal). But many objections to proportional fonts boil down to monospaced fonts being presumed to be better simply because either</p> <ul> <li>people are used to them, or</li> <li>some \u2013 arguably brittle and source diff-unfriendly \u2013 code alignment practices, such as hanging comments, assume that the code will always be displayed using a monospaced font.</li> </ul> <p>If you\u2019re completely unconvinced about using proportional fonts for coding then fine; this article is not for you.</p> <p>If, on the other hand, you are interested then there is one common objection to proportional fonts that does need to be addressed, which is that many (sans serif) fonts do not reliably distinguish between characters that matter when writing code. (In contrast, almost all monospaced fonts do.) So let\u2019s tackle this first.</p>"},{"location":"2024-01/lisnoti-a-proportional-font-that-works-for-coding-too/#the-a1-ai-called-al","title":"The A1 AI called Al","text":"<p>I am going to focus on sans serif fonts because I think these are overall clearer for coding than serif fonts (e.g. Times New Roman).</p> <p>The most obvious problem when using sans serif fonts for coding is distinguishing</p> <ul> <li>upper case <code>I</code> from lower case <code>l</code> (and while we\u2019re here, also the number <code>1</code>), and</li> <li>upper case <code>O</code> from zero <code>0</code>. </li> </ul> <p>For instance, Gill Sans () has the classic purist design where <code>I</code>, <code>l</code> and <code>1</code> are indistinguishable.</p> <p>Many sans serif fonts do at least distinguish the number <code>1</code> from letters, e.g Roboto ( \u2013 Arial and Calibri are similar). But this still leaves <code>I</code> and <code>l</code> easily confused. </p> <p>Legibility is a wider concern</p> <p>The need to distinguish similar-looking characters such as <code>I</code> v <code>l</code> is not just a coding issue \u2013 see e.g. this Linotype article on redesigning Frutiger to comply with German legibility standard DIN 1450.</p> <p>You may find it surprising that many typefaces commonly used for \u2013 sometimes even specifically designed for \u2013 public signage do not reliably distinguish between the letters <code>I</code> and <code>l</code>. (You can probably spot a few in this summary.)</p> <p>Notable historical exceptions include the pioneering London Underground Johnston typeface, the UK\u2019s Transport typeface and Germany\u2019s DIN 1451.</p> <p>To address the <code>I</code> v <code>l</code> issue, modern fonts often follow in the footsteps of Johnston (see the boxout) and put a tail on the lower case <code>l</code>. Unfortunately, there is an alternative convention whereby the lower case <code>l</code> remains tailless and instead serifs are added to the upper case <code>I</code>. An example of this inconsistency is the <code>I</code> and <code>l</code> typography in Google\u2019s Noto global fonts project where</p> <ul> <li> <p>the Japanese version of Noto Sans uses the first convention (), but</p> </li> <li> <p>the Western version of Noto Sans uses the second convention ().</p> </li> </ul> <p>What this means is that, even if <code>I</code> is distinct from <code>l</code> within a font, you cannot deduce which is which in isolation. Plus there is additional cognitive load.</p> <p>Back at the start of 2017, Paul McAulay asked that Noto Sans and Roboto be modified to correct precisely this ambiguity (see here for the Roboto request), but he and others were given pretty short shrift. The objection that I really struggle to understand is that making <code>I</code> and <code>l</code> distinct cannot be done for style reasons \u2013 see for example this classic (\u2018it just won\u2019t fit\u2019) and, in relation to a different font,  a dismissal based solely on style (\u2018differentiation is not a design goal\u2019).</p> <p>A limited few proportional fonts do handle the <code>I</code> v <code>l</code> issue:</p> <ul> <li>David Jonathan Ross\u2019s proportionally-spaced sans serif variant of his Input font does this by design, although I find it too clunky for general typography (which to be fair is not what it is intended for).</li> <li>I suspect IBM Plex achieves this accidentally solely because of the <code>I</code> in the IBM logo.</li> </ul> <p>But fixing the <code>I</code> v <code>l</code> issue is just the start\u00a0\u2026</p>"},{"location":"2024-01/lisnoti-a-proportional-font-that-works-for-coding-too/#the-long-wish-list","title":"The long wish list","text":"<p>As well as being suitable for writing standard computer code and as a general font, I\u2019d also like a font that includes mathematical and other symbols that crop up in code documentation and, if you use Julia, in actual code.</p> <p>The reason you want all characters in the same font is that otherwise the environment you\u2019re using will either show a meaningless replacement character (e.g. <code>\ufffd</code>) or make a \u2013 likely garish \u2013 substitution from an unrelated font.</p> <p>Hyphen vs minus sign</p> <p>Using hyphen to represent a minus sign is a frequently-occurring typographical sin, the more so because almost all modern fonts specifically include an actual minus sign (<code>U+2212</code>).</p> <p>Even the generally reliable Matthew Butterick lapses (albeit while advocating the use of true maths symbols) by suggesting en dash as a minus sign \u2013 please don\u2019t do that!</p> <p>Even professional typographers screw this up. At time of writing, Noto Sans does implement the Unicode minus sign but does so incorrectly as a hyphen. (The minus sign in the font file is literally a reference to the hyphen glyph.)</p> <p>Here\u2019s my personal long list of font requirements and character coverage:</p> <ol> <li> <p>Reliable distinction of <code>I</code>/<code>l</code> and <code>O</code>/<code>0</code> (as already discussed).</p> </li> <li> <p>Consistent arithmetic, comparison, logic, set, n-ary and other common mathematical operators and entities, e.g. </p> <ul> <li>arithmetic: <code>\u2212 \u00d7 \u00f7 \u00b1 \u2213 \u221e</code></li> <li>comparison: <code>\u2264 \u2260 \u2265 \u2248 \u2261 \u2262 \u221d</code></li> <li>logic: <code>\u00ac \u2227 \u2228 \u22bb \u22a4 \u22a5 \u22a6</code></li> <li>set: <code>\u2229 \u222a \u2208 \u2209 \u2282 \u2283 \u2286 \u2287 \u2205</code></li> <li>n-ary: <code>\u2211 \u220f \u22c0 \u22c1  \u22c2 \u22c3</code></li> <li>other: <code>\u222b \u2202 \u221a \u0394  \u2207 \u2200 \u2203</code></li> </ul> </li> <li> <p>Greek and Cyrillic letters \u2013 mathematics and logic make frequent use of Greek letters and occasionally Cyrillic ones too.</p> </li> <li> <p>Consistently formatted digit and Roman letter superscripts and subscripts, e.g. <code>\u00b9\u00b2\u00b3 \u207d\u207e\u207a\u207b \u1d43\u1d47\u1d9c \u1d2c\u1d2e\u1d30</code> and <code>\u2081\u2082\u2083 \u208d\u208e\u208a\u208b \u2090\u2091\u2095</code>.</p> </li> <li> <p>A selection of symbols (consistently including all related characters within each set \u2013 I\u2019ve shown only subsets below)</p> <ul> <li>squares, diamonds, rectangles, triangles, circles and stars, e.g.<code>\u25a0 \u25c7 \u25ae \u25b3 \u25cf \u2606</code></li> <li>arrows, e.g. <code>\u2192 \u21d2 \u21e8 \uffeb</code></li> <li>ticks and crosses, e.g. <code>\u2611 \u2714\u2715\u2718</code></li> <li>box drawing, e.g.<code>\u2502 \u2510 \u252c \u251c</code></li> <li>game characters, e.g.<code>\u265a\u2655\u2660\u2661</code></li> <li>miscellaneous but useful, e.g. <code>\u2638 \u266f \u2318 \u2423</code></li> </ul> </li> <li> <p>All the operators parsed by Julia.</p> </li> <li> <p>Unicode mathematical alphanumeric symbols, e.g. <code>\ud835\udc00\ud835\udc34\ud835\udc68 \ud835\udc9c\ud835\udcb2\ud835\udcd0 \ud835\udd04 \ud835\udd38 \ud835\udd6c \ud835\udda0\ud835\uddd4\ud835\ude08\ud835\ude3c \ud835\ude70</code> (noting that these too can be parsed by Julia).</p> </li> </ol> <p>All the above but monospaced</p> <p>If you want all the above but you\u2019re happy with a monospaced font, then I suggest you check out Julia Mono.</p>"},{"location":"2024-01/lisnoti-a-proportional-font-that-works-for-coding-too/#introducing-the-lisnoti-font","title":"Introducing the Lisnoti font","text":"<p>Google\u2019s Noto global fonts project has generated sets of sans serif fonts that, in combination, have excellent Unicode coverage and are published under an open source licence. This means that we can build on the huge amount of technically skilled typographical work that has already been done and publish the results for general use (provided original copyright statements, reserved font name declarations and the original licence text are included).</p> <p>So that\u2019s what I\u2019ve done \u2013 I\u2019ve edited and combined Noto Sans fonts to create a proportional font Lisnoti (<code>/l\u026az\u02c8n\u0259\u028ati\u02d0/</code>) that meets all the requirement listed in this article, including, of course, fixing the ambiguous characters we kicked off with ().</p> <p>Lisnoti can be downloaded from GitHub here (where you can also find further detail on its construction). </p> <p>It is available </p> <ul> <li>in regular, italic, bold and bold-italic variants, and</li> <li>in OpenType (<code>.ttf</code>) and web (<code>.woff</code> and <code>.woff2</code>) formats,</li> </ul> <p>and is published under the SIL Open Font Licence (OFL). </p> <p>I\u2019ve been using Lisnoti in Visual Studio and VS Code for some time now, and it works for me. (I\u2019ve tried it on Windows and Apple, but not on Linux.)</p> <p>If you have comments on Lisnoti, please use the Lisnoti GitHub discussions page. If you do comment then please bear in mind that I am not a typography expert, just a frustrated user.</p> <p>Cheers</p>"},{"location":"2024-10/on-contemporary-mortality-models-for-actuarial-use/","title":"On contemporary mortality models for actuarial use","text":"<p>Stephen Richards\u2019 and Angus Macdonald\u2019s paper \u2018On Contemporary Mortality Models for Actuarial Use\u2019 (due to be discussed at the Institute of Actuaries on 24 October 2024) makes the case for the following in mortality experience analysis<sup>1</sup>:</p> <ol> <li> <p>Use individual data if at all possible.</p> </li> <li> <p>Work in continuous time and use instantaneous mortality rates, i.e. \u03bc rather than q.</p> </li> <li> <p>Consider mortality experience data as comprising a series of Bernoulli trials over infinitesimally small time periods.</p> </li> </ol> <p>The paper could be read as a polemic against actuaries who can\u2019t help but think in terms of q and whose first instinct is to group all time and age-dependent data on annual grids. Which is fine by me \u2013 I agree with the thrust of the paper and, in particular, the above three points.</p> <p>So, having welcomed the paper, I do have a few observations\u2026</p>"},{"location":"2024-10/on-contemporary-mortality-models-for-actuarial-use/#observations","title":"Observations","text":""},{"location":"2024-10/on-contemporary-mortality-models-for-actuarial-use/#1-are-actuaries-really-migrating-from-q-to","title":"1. Are actuaries really migrating from\u00a0q\u00a0to\u00a0\u00b5?","text":"<p>The paper\u2019s introduction states that \u2018\u00b5<sub>x</sub> is usurping q<sub>x</sub> in actuarial work.\u2019 While I\u2019d like this to be true, I can\u2019t help but wonder whether it\u2019s wishful thinking.</p> <p>Trying to switch the CMI Mortality Projections Model from q to \u00b5<sup>2</sup></p> <p>As chair of the CMI\u2019s Mortality Projections Committee, I oversaw the revision of its Projections Model in 2016. I was keen to remove all q-based aspects inherited from the previous approach, but \u2013 relevant in this context \u2013 all changes from q to \u00b5 were opposed by at least some actuaries. </p> <p>The following two changes were objected to<sup>3</sup>, but did gain majority support in consultation and make it into the final version:</p> <ul> <li> <p>Changing the \u2018long-term rate of mortality improvement\u2019 (LTR) from q to \u00b5-based.</p> </li> <li> <p>Changing the projections part of the model to work in directly in terms of \u00b5 rather than q.</p> </li> </ul> <p>The only q-related aspect remaining was that the improvement factors output by the Model are expressed in terms of q-multipliers, but the committee itself felt that changing this to \u00b5 was a step too far<sup>4</sup>.</p> <ul> <li> <p>Many actuaries simply prefer q (in the same way that they prefer annually-compounded interest rates to the force of interest). See the boxout for a documented example.</p> </li> <li> <p>Like it or not, q is the current lingua franca in the sense that \u2018110% of S4PxA\u2019 without further qualification means scale q by 110%<sup>5</sup>.</p> </li> <li> <p>In my experience, most actuarial valuation systems work with q, data grouped by age and annual time grids<sup>6</sup>.</p> </li> <li> <p>Even within specialised mortality experience analysis environments, where, as the paper notes, there are clear benefits to working in continuous time, I\u2019ve encountered plenty of systems that use q.</p> </li> </ul>"},{"location":"2024-10/on-contemporary-mortality-models-for-actuarial-use/#2-generalising-survival-likelihood-factorisation","title":"2. Generalising survival likelihood factorisation","text":"<p>Section\u00a03.3 of part\u00a0II of the paper notes that survival likelihood can be factorised over sub-time intervals and that likelihoods for contiguous intervals can validly be multiplied together.</p> <p>It\u2019s worth highlighting that there is a more general concept at play here, which is that (a)\u00a0the count of actual deaths, (b)\u00a0the sum over individuals of integrals of mortality rate over exposure periods and (c)\u00a0log-likelihood are all measures over experience data<sup>7</sup>. This means that they</p> <ul> <li> <p>give the same result when summed over a partitioned experience dataset regardless of how it is partitioned, and</p> </li> <li> <p>in particular, when summing over multiple experience datasets, there is no need for experience time periods to be contiguous \u2013 the sole requirement is that experience datasets do not intersect.</p> </li> </ul> <p>The above are two points are intuitively well understood by practitioners<sup>8</sup>; my point is that the maths \u2013 suitably defined \u2013 aligns with this.</p>"},{"location":"2024-10/on-contemporary-mortality-models-for-actuarial-use/#3-likelihood-vs-log-likelihood","title":"3. Likelihood vs log-likelihood","text":"<p>The technical part\u00a0II of paper focuses on likelihood, i.e. probability, but I\u2019d argue that log-likelihood is both more tractable and more fundamental. These are subjective concepts, so this is personal opinion, not a matter of right or wrong.</p> <p>Let\u2019s take tractability first.</p> <ul> <li> <p>The paper is littered<sup>9</sup> with product signs, i.e. \\(\\prod\\), arising from the use of probabilities. In contrast, working in term of log-likelihood results in standard integrals and makes linearity self-evident where present<sup>10</sup>. As a case in point, I didn\u2019t find the continued use of the Volterra product integral, i.e.</p> <pre>\\[\\prod_a^b\\Big(1-f(t)\\,\\text{d}t\\Big)\\]</pre> <p>enlightening. Given that they are technically equivalent, I\u2019d suggest it\u2019s simpler to explain survival probabilities once, then take logs and work in terms of standard integrals, i.e.</p> <pre>\\[-\\int_a^b \\! f(t) \\,\\text{d}t\\]</pre> </li> <li> <p>In terms of what is actually practical (which is a form of tractability), implementing mortality experience analysis solely using probability will typically fail because of numerical underflow. So, if you believe that code should map to concepts, the concept you want is log-likelihood.</p> </li> </ul> <p>Turning to fundamentality:</p> <ul> <li> <p>A key rationale for fitting models by maximising log-likelihood and selecting between them using the AIC is information theory, the core unit of which is information content, which in turn is defined as (negative) log-likelihood.</p> </li> <li> <p>The paper repeatedly refers to the concept of infinitesimal Bernoulli trials. I couldn\u2019t agree more, but the usual mathematical treatment of summed infinitesimals requires something akin to dL to make sense, which works only if L is log-likelihood rather than plain likelihood.</p> </li> <li> <p>Observation\u00a02 above (that there is a more general concept at play than just being able to factorise the survival probabilities over contiguous time periods) is best expressed in terms of measures, which means using log-likelihood.</p> </li> </ul>"},{"location":"2024-10/on-contemporary-mortality-models-for-actuarial-use/#conclusion","title":"Conclusion","text":"<p>Stephen Richards\u2019 and Angus Macdonald\u2019s paper is to be welcomed for making the case for using continuous time and instantaneous rates in mortality experience analysis.</p> <p>If it results in even one actuary thinking before diving in with a q-based approach, then that\u2019s a good thing.</p> <ol> <li> <p>The paper\u2019s title suggests it covers all mortality modelling, but its sole focus is the analysis of portfolio mortality experience. So, for instance, mortality projection modelling is omitted entirely. This doesn\u2019t detract from the paper\u2019s contents \u2013 mortality experience analysis is an important topic in itself.\u00a0\u21a9</p> </li> <li> <p>Technically the CMI Mortality Projections Model is defined in terms of m, not \u00b5, but it is m rather than q that maps straightforwardly to \u00b5.\u00a0\u21a9</p> </li> <li> <p>See Q\u202f6.1 and Q\u202f6.2 in WP\u00a093.\u00a0\u21a9</p> </li> <li> <p>This is not a criticism \u2013 the CMI has an obligation to its users to consider the stability and consistency of its outputs.\u00a0\u21a9</p> </li> <li> <p>And gloss over how to treat ages for which \\(q_x\\ge1\\) !\u00a0\u21a9</p> </li> <li> <p>There are valid practical arguments for, as well as against, this approach.\u00a0\u21a9</p> </li> <li> <p>Log-likelihood is a signed measure.\u00a0\u21a9</p> </li> <li> <p>For instance, no-one would bat an eyelid (in a technical sense) if mortality experience data are analysed excluding pandemic-affected periods, resulting in non-contiguous time periods.\u00a0\u21a9</p> </li> <li> <p>A quick search suggests there are over 60 product signs, i.e. \\(\\prod\\), in the paper.\u00a0\u21a9</p> </li> <li> <p>It is for good reason that logarithms were originally described as mirifici.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-08/mortality-measures-matter/","title":"Mortality: Measures matter","text":"<p>This is the first in a series of articles outlining mortality experience analysis fundamentals, by which I mean estimating the underlying mortality for individuals in a defined group (e.g. members of a pension plan) from mortality experience data.</p> <p>This will be fairly technical, but I\u2019ll aim</p> <ul> <li>to be concise,</li> <li>to pull out the key insights, including what does and doesn\u2019t matter in practice, and</li> <li>to map concepts one-to-one to the process of actually carrying out a mortality experience analysis or calibrating and selecting mortality models.</li> </ul> <p>A lot of this is reasonably well known, but it is not always available in one place or easily accessible, and sometimes key points are omitted. It won\u2019t be all plain sailing, and there\u2019ll be a few potentially contentious and maybe even surprising points along the way.</p> <p>In this first article, I\u2019ll set out the foundations.</p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol>"},{"location":"2025-08/mortality-measures-matter/#definitions","title":"Definitions","text":"<p>An exposed-to-risk (E2R) for an individual comprises<sup>1</sup><sup>2</sup></p> <ul> <li>an exposure period \\([\\nu,\\tau)\\) throughout which the individual was alive, and</li> <li>an indicator \\(\\delta\\), which is \\(1\\) if the individual died at \\(\\tau\\) or else \\(0\\).</li> </ul> <p>I\u2019ll write an E2R as \\(\\varepsilon=(\\nu,\\tau,\\delta)\\).</p> <p>For each individual we also have a set of facts, \\(i\\), known at the time of the analysis and which are time invariant.</p> <p>If a fact relates to something that would have happened except that the individual died, e.g. pension in payment in 2025 for an individual who died in 2024, then we assume this fact is determined as if the individual had lived. This is critical for unbiased analysis and is sometimes referred to as \u2018the principle of correspondence\u2019<sup>3</sup>.</p> <p>An experience dataset \\(\\text{Exp}\\) comprises pairs of facts and E2Rs, i.e. \\(\\{(i,\\varepsilon)\\}\\) for which no E2Rs for the same individual overlap in time<sup>4</sup>.</p> <p>A variable is a real-valued function \\(f(i,t)\\) of facts \\(i\\) and time \\(t\\)<sup>5</sup>. A variable is not random \u2013 by assumption, the sole source of stochasticity is whether individuals die or not, which is embedded in \\(\\varepsilon\\).</p> <p>A mortality, \\(\\mu\\), is a strictly positive variable<sup>6</sup> that specifies the probability of an individual dying over an infinitessimal time interval \\(\\text{d}t\\) as \\(\\mu(i,t)\\,\\text{d}t\\), i.e. an independent<sup>7</sup> Bernoulli trial.</p>"},{"location":"2025-08/mortality-measures-matter/#what-is-random","title":"What is random?","text":"<p>We will assume that the mortality \\(\\mu\\) at hand is itself completely deterministic and the sole source of random variation is whether or not individuals die according to \\(\\mu\\).</p> <p>Don\u2019t lose sight of the fact that this is a convenient fiction to make the problem tractable and that, in practice, this is never true for mortality data, not least because \\(\\mu\\) itself is also stochastic. This manifests as observed variances being materially higher than would be predicted by a fitted Poisson distribution, and is known as overdispersion.</p> <p>Insight 1. Always allow for overdispersion</p> <p>If you don\u2019t allow for overdispersion then you will underestimate uncertainty and overfit models.</p> <p>[All mortality insights]</p> <p>The good news is that we can fix things up to allow for overdispersion, which we\u2019ll get to in a later article. In the meantime, whenever you come across references to mortality variance or uncertainty (in this blog or anywhere else), a little voice in your head should be saying \u2018remember to allow for overdispersion\u2019.</p>"},{"location":"2025-08/mortality-measures-matter/#the-measures-that-matter","title":"The measures that matter","text":"<p> We can picture experience data as comprising infinitesimals \\(\\text{d}(i,\\varepsilon)\\) that can be added up in a couple of different ways. The mathematical approach to this is to define measures on the data. The pay off is that provided we use a measure we can add up functions over experience data any way we like and we\u2019re guaranteed to end up with the same answer.<sup>8</sup> </p> <p>Insight 2. Experience data is \u2018measurable\u2018</p> <p>Provided we use measures, we\u2019ll always get the same answer regardless of how an experience dataset is partitioned.</p> <p>In particular, there is no need</p> <ul> <li>for experience time periods to be contiguous<sup>9</sup> \u2013 the sole requirement is that elements of the experience datasets do not intersect, or</li> <li>to track individuals across experience datasets relating to different time periods<sup>10</sup>.</li> </ul> <p>[All mortality insights]</p> <p>I suggested above that there are two things we want to add up. If you\u2019re a practitioner, you\u2019ve already come across them (or at least an approximation to them) and, as we\u2019ll see over this series, pretty much every useful aspect of mortality experience analysis can be expressed directly in terms of them:</p> <p> <ol> <li> <p>Actual deaths is the sum of \\(f\\) over deaths at time of death:</p> <pre>\\[\\text{A}f=\\sum_{(i,\\varepsilon)\\in \\text{Exp}}\\delta_\\varepsilon f(i,\\tau_\\varepsilon)\\]</pre> <p>Note that the variable, \\(f\\), is evaluated at time of death.</p> </li> <li> <p>Expected deaths<sup>11</sup> with respect to mortality \\(\\mu\\) is the integral of \\(\\mu\\) times \\(f\\) over all exposure periods:</p> <pre>\\[\\text{E} f=\\sum_{(i,\\varepsilon)\\in \\text{Exp}}\\int_{\\nu_\\varepsilon}^{\\tau_\\varepsilon}\\!\\mu(i,t)f(i,t)\\,\\text{d}t\\]</pre> </li> </ol> <p></p> <p>\u2018Expected\u2019 is not expectation</p> <p>\\(\\text{E}f\\) is a random variable (like \\(\\text{A}f\\)), and so describing it as \u2018expected\u2019 deaths can give rise to confusion. But the practice is so ensconced that using a different term would be even more confusing.</p> <p>The terminology arises because \u2018expected deaths\u2019 is typically an estimate of expected deaths over short but finite time intervals, e.g. years. The definition of \\(\\text{E}\\) here is the same except it is defined on a grid of infinitesimal time intervals, which discards the complexity of determining survival during finite intervals and makes this the canonical definition.</p> <p>For avoidance of doubt, I\u2019ll always use \\(\\mathbb{E}\\) to refer to true expectation.</p> <p>On notation and terminology:</p> <ul> <li>The dataset \\(\\text{Exp}\\) and, for \\(\\text{E}\\), the mortality \\(\\mu\\) are typically implicit from context \u2013 it is rare that we need additional notation to make them explicit. But, for the avoidance of doubt, \\(\\text{E}\\) always implies a background mortality \\(\\mu\\).</li> <li>There is a multitude of notations for integrating using measures (see e.g. here), of which \\(\\int \\!f(x)\\,\\text{M}(\\text{d}x)\\) and \\(\\int \\!f\\,\\text{dM}\\) are common. But the simplest is \\(\\text{M}f\\), which is what I\u2019ll use.</li> </ul> <p>Although I\u2019ve emphasised that \\(\\text{A}\\) and \\(\\text{E}\\) are measures over experience data, I confess that I don\u2019t use this terminology day-to-day<sup>12</sup>. In fact, I\u2019ve hardly heard anyone mention \u2018measures\u2019 in connection with experience analysis, which may explain why their importance seems to be overlooked and why some practitioners end up confused over the meaning of \u2018expected\u2019 (see box out), or whether individuals need to be tracked throughout the experience data.</p> <p>Insight 3. The continuous time definitions of A and E are canonical</p> <p>The continuous time definitions of \\(A\\) and \\(E\\) are measures and the canonical definitions of actual and expected deaths.</p> <p>Other definitions can lead to confusion \u2013 usually over \\(\\text{E}\\) vs true expectation \u2013 and spurious complexity.</p> <p>[All mortality insights]</p>"},{"location":"2025-08/mortality-measures-matter/#why-include-f","title":"Why include\u00a0f\u202f?","text":"<p>References to actual and expected deaths in mortality analyses are often written simply as \\(A\\) and \\(E\\), so why do we have a variable in our definitions?</p> <p>A simple justification<sup>14</sup> is that real world mortality work requires weighted statistics<sup>13</sup>:</p> <ol> <li> <p>It is standard to analyse actual and expected deaths weighted by benefit amount (\u2018amounts-weighted\u2019) as well as unweighted (\u2018lives-weighted\u2019). So including \\(f\\) means that we have this requirement covered.</p> </li> <li> <p>We may<sup>15</sup> want to weight data by its relevance (also known as reliability or importance).</p> </li> </ol> <p>The fundamental reason though is that, as noted above, pretty much every useful aspect of mortality experience analysis can be expressed directly in terms of \\(\\text{A}f\\) and \\(\\text{E}f\\).</p> <p>Next article: A over E</p> <p>In the next article I\u2019ll review the properties of \\(\\text{A}f\\) and \\(\\text{E}f\\) and their role in A/E analysis.</p> <ol> <li> <p>I\u2019ll take it as a given that we should work in (some representation of) continuous time if at all possible. Otherwise we\u2019d be (a)\u00a0throwing away data and (b)\u00a0creating additional cognitive load and potentially biased or even plain wrong results by having to make assumptions about averages.\u00a0\u21a9</p> </li> <li> <p>The notation \\([\\nu,\\tau)\\) means the interval \\(\\{t\\in\\mathbb{R} \\mid  \\nu\\le t \\lt \\tau\\}\\).\u00a0\u21a9</p> </li> <li> <p>From a technical point of view, this principle also means mortality models can\u2019t cheat simply by looking at the data.\u00a0\u21a9</p> </li> <li> <p>Easy to stipulate in theory, but data de-duplication is an essential and sometimes non-trivial part of real world mortality experience analysis.\u00a0\u21a9</p> </li> <li> <p>We need to place some conditions on the dependence of a variable \\(f(i,t)\\) on time \\(t\\).</p> <p>The most general is that \\(f(i,t)\\) is left-continuous with right limits in \\(t\\), left continuity being required so that the value at death is consistent with the value in the immediately preceding exposure. But this level of generality is impractical for an actual implementation.</p> <p>A more useful real-world condition is that \\(f(i,t)\\) is smooth in \\(t\\) at the scale of numerical integration. We\u2019ll leave smoothness undefined for now, other than to state that, as a minimum, it implies absolute continuity in \\(t\\).\u00a0\u21a9</p> </li> <li> <p>An implementation would also need a mortality to specify a terminal date or age by individual (because mortality tables stop), but we don\u2019t need that for this exposition.\u00a0\u21a9</p> </li> <li> <p>For the avoidance of doubt, we assume that these Bernoulli trials are independent by time and by individual.\u00a0\u21a9</p> </li> <li> <p>The freedom to partition experience data may also present opportunities to run calculations in parallel. (I suggest that your mortality experience calculations should be running in parallel at least somewhere along the line.)\u00a0\u21a9</p> </li> <li> <p>An obvious example is excluding mortality experience from the height of the COVID-19 pandemic, potentially resulting in non-contiguous data from before and after the excluded time period.\u00a0\u21a9</p> </li> <li> <p>Tracking individuals across experience datasets for different time periods may however be a very sensible data check.\u00a0\u21a9</p> </li> <li> <p>We could alternatively define an \u2018exposure measure\u2019 as</p> <pre>\\[\\text{S} f=\\sum_{(i,\\varepsilon)\\in \\text{Exp}}\\int_{\\nu_\\varepsilon}^{\\tau_\\varepsilon}\\!f(i,t)\\,\\text{d}t\\]</pre> <p>While this is a simpler definition, it would</p> <ul> <li>clutter up our notation because we\u2019d end up writing \\(\\text{S}\\mu f\\) everywhere instead of \\(\\text{E}f\\), and</li> <li>obscure the symmetry between \\(\\text{A}\\) and \\(\\text{E}\\).</li> </ul> <p>\u21a9</p> </li> <li> <p>I think I usually describe \\(A\\) and \\(E\\) as \u2018linear operators\u2019.\u00a0\u21a9</p> </li> <li> <p>Note that \\(E\\) can no longer serve as an estimate of its own variance when dealing with weighted statistics.\u00a0\u21a9</p> </li> <li> <p>Another potential justification is that it is mathematically convenient to use \\(f\\in \\{0,1\\}\\) as an indicator of dataset membership by time. Unfortunately, this is an implementation nightmare in its full generality, and so has limited real-world value. There are better approaches to achieving this in practice.\u00a0\u21a9</p> </li> <li> <p>And in a future article will.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-08/mortality-a-over-e/","title":"Mortality: A over E","text":"<p>Why \u2018A over E\u2019?</p> <p>\u2018A over E\u2019 literally refers to \u2018actual\u2019 deaths divided by \u2018expected\u2019 deaths as a measure of how experience data compares with a mortality.</p> <p>In practice, \u2018A over E\u2019 is often interpreted as meaning the whole statistical caboodle, which is how I\u2019ll use it here.</p> <p>In the previous article we defined experience data, variables and mortality with respect to that data, and the \\(\\text{A}\\) (actual) and \\(\\text{E}\\) (expected) deaths operators.</p> <p>In this article we\u2019ll put \\(\\text{A}\\) and \\(\\text{E}\\) to work.</p>"},{"location":"2025-08/mortality-a-over-e/#the-expectation-result","title":"The \u2018expectation\u2019 result","text":"<p>When the mortality \\(\\mu\\) is the true mortality then for any variable \\(f\\) it is easy to show that</p> <pre>\\[\\mathbb{E}\\big(\\text{A}f-\\text{E}f\\big)=0\\tag{1}\\]</pre> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol> <p>where \\(\\mathbb{E}\\) is true expectation (allowing for probabilities of survival etc).</p> <p>If is worth emphasising that this is true</p> <ul> <li>for any weight, even if that that weight was used to fit the mortality in question, and</li> <li>for any subset of the experience data, provided the choice of subset does not depend on E2R information.</li> </ul> <p>This bears repeating: it doesn\u2019t matter how you calibrated your mortality model, if your model works then the expected value of \\(\\text{A}f-\\text{E}f\\), whether it be weighted by lives<sup>1</sup>, by amounts or by any other variable you or I may choose, is zero.</p> <p>Insight 4. The expected value of A\u2212E is zero</p> <p>If \\(\\mu\\) is the true mortality then the expected value of \\(\\text{A}f-\\text{E}f\\) is zero, i.e.</p> <pre>\\[\\mathbb{E}\\big(\\text{A}f-\\text{E}f\\big)=0\\]</pre> <ul> <li>for any variable \\(f\\) (even if \\(f\\) was used to fit the mortality in question), and</li> <li>for any subset of the experience data (provided the choice of subset does not depend on E2R information).</li> </ul> <p>[All mortality insights]</p>"},{"location":"2025-08/mortality-a-over-e/#the-variance-result","title":"The \u2018variance\u2019 result","text":"<p>Observations are noisy and so we\u2019d also like to calculate the variance to assess how close the observed result is to \\(0\\).</p> <p>When the mortality \\(\\mu\\) is the true mortality then for any variable \\(f\\) then we can show that</p> <pre>\\[\\text{Var}\\big(\\text{A}f-\\text{E}f\\big)=\\mathbb{E}\\big(\\text{E}f^2\\big) \\tag{2}\\]</pre> <p>Some observations:</p> <ol> <li> <p>This is a generalisation of the lives-weighted (or unweighted) result<sup>1</sup> that the variance of A\u2212E is E.</p> </li> <li> <p>If you have the machinery in place to calculate \\(\\text{A}f\\) and \\(\\text{E}f\\) then you are also immediately in a position to estimate the variance of \\(\\text{A}f-\\text{E}f\\) because it\u2019s just \\(\\text{E}g\\) where \\(g=f^2\\).</p> </li> <li> <p>This does not allow for overdispersion and so is typically an underestimate of variance. I\u2019ll ignore this for the time being on the basis that I\u2019ll cover overdispersion later in this series.</p> </li> </ol> <p>Insight 5. The same machinery that defines A\u2212E can be used to estimate its uncertainty</p> <p>If \\(\\mu\\) is the true mortality then the variance of \\(\\text{A}f-\\text{E}f\\) equals the expected value of \\(\\text{E}f^2\\), i.e.</p> <pre>\\[\\text{Var}\\big(\\text{A}f-\\text{E}f\\big)=\\mathbb{E}\\big(\\text{E}f^2\\big)\\]</pre> <p>(This is before allowing for overdispersion.)</p> <p>Caveat: \\(f\\) is an ad hoc reallocation of log-likelihood; it is not relevance. For the version of this insight that does take account of relevance, see Insight\u00a017.</p> <p>[All mortality insights]</p> <p>Familiarity with lives-weighted case can lead practitioners astray in relation to weighted statistics:</p> <ul> <li> <p>An unadjusted lives-weighted variance estimate should not be used to estimate the variance of a weighted A/E because it will always understate it<sup>2</sup>.</p> </li> <li> <p>Hard-coding the assumption that all statistics are lives-weighted into your system will mean your system will struggle when weighted results are required in the future.</p> </li> <li> <p>When experience data is provided in the form of grouped deaths and exposures, it is reasonably common for this data to be provided weighted both by lives and by benefit amount. But what is almost always overlooked<sup>3</sup> is that the amounts-weighted grouped experience data should in addition include exposure data weighted by amount squared. In other words, grouped weighted data should comprise \\(\\text{A}f\\), \\(\\text{E}f\\) and \\(\\text{E}f^2\\) as opposed to just \\(\\text{A}f\\) and \\(\\text{E}f\\).</p> </li> </ul>"},{"location":"2025-08/mortality-a-over-e/#diagnostics","title":"Diagnostics","text":"<p>With a mean and a variance to hand, we are in a position to take a first stab at A/E diagnostics, i.e. residuals and  confidence intervals.</p> <p>The Pearson residual is (actual\u00a0\u2212\u00a0expected) / (estimated\u00a0standard\u00a0deviation). In our case, actual is \\(\\text{A}f-\\text{E}f\\), expected is \\(0\\), and estimated standard deviation is \\(\\sqrt{\\text{E}f^2}\\), and so the Pearson residual is</p> <pre>\\[r_\\text{P}=\\frac{\\text{A}f-\\text{E}f}{\\sqrt{\\text{E}f^2}}\\sim N\\!\\left(0,1\\right)\\]</pre> <p>Another obvious A/E diagnostic is the literal one \u2013 appeal to the central limit theorem and the low variance of \\(\\text{E}f\\) to assume</p> <pre>\\[\\frac{\\text{A}f}{\\text{E}f}\\sim N\\!\\left(1,\\;\\frac{\\text{E}f^2}{(\\text{E}f)^2}\\right)\\]</pre> <p>In other words, review \\(\\text{A}f/\\text{E}f\\) and compare its difference from \\(1\\) with (a multiple of) \\(\\pm\\sqrt{\\text{E}f^2} / \\text{E}f\\).</p> <p>Insight 6. A/E variance increases with concentration</p> <p>\\(\\sqrt{\\text{E}w^2} / \\text{E}w\\), where \\(w\\ge0\\) is a useful and recurring measure of effective concentration in relation to mortality uncertainty. It implies that the more concentrated the experience data (in some sense) then the greater the variance of observed mortality.</p> <p>Using unweighted variance without adjustment to estimate weighted statistics will likely understate risk.</p> <p>[All mortality insights]</p> <p>The above diagnostics are fine in practice, but they have some nagging drawbacks:</p> <ul> <li>We\u2019ve relied on \\(\\sqrt{\\text{E}f^2}\\gg \\text{E}f\\) and the central limit theorem, which will break down for datasets with fewer or more concentrated weighted deaths.</li> <li>The implied confidence intervals can include negative values!</li> </ul> <p>Next article: Log-likelihood</p> <p>We can do (a bit) better and so we\u2019ll revisit A/E diagnostics in due course. But in order to do that we\u2019ll need to define the log-likelihood, which will be the subject of my next article.</p> <ol> <li> <p>I\u2019ll interpret lives-weighted as meaning \\(f\\in\\{0,1\\}\\), which is a little more general than unweighted, which is \\(f=1\\). In both cases \\(f^2=f\\) and hence \\(\\text{E}f^2=\\text{E}f\\).\u00a0\u21a9\u21a9</p> </li> <li> <p>Provided \\(f\\) exhibits some variation over the experience data, it is a mathematical truth that \\(\\text{E}f^2\\cdot\\text{E}1\\gt (\\text{E}f)^2\\).\u00a0\u21a9</p> </li> <li> <p>By parties that I think should know better.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-08/mortality-log-likelihood/","title":"Mortality: Log-likelihood","text":"<p>I think it\u2019s a shame that the \u2018log\u2019 in \u2018log-likelihood\u2019 is so often presented as a technical convenience or a device for avoiding numerical under/overflow. Yes, it is definitely both of these things, but it is much more fundamental.</p> <p>Expected log-probability, i.e. entropy, lies at the heart of information theory. And the concept of entropy itself is pervasive, having extended beyond thermodynamics, its original home, into quantum physics and general relativity, as well as information theory.</p> <p>So, without further ado, let\u2019s define log-likelihood for mortality experience data.</p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol>"},{"location":"2025-08/mortality-log-likelihood/#definition","title":"Definition","text":"<p>Although we can approach log-likelihood from an all-of-time point of view, I think it\u2019s instructive to start at the opposite end of the scale, with infinitesimals<sup>1</sup>.</p> <p>Over an infinitesimal time period \\(\\text{d}t\\), the probability of survival is</p> <pre>\\[p\\approx\\exp\\Big\\{\\!-\\!\\mu(i,t)\\text{d}t\\Big\\}\\]</pre> <p>There are two possible outcomes, which I\u2019ll represent using \\(\\delta\\), which is \\(1\\) if the individual died and \\(0\\) otherwise. The likelihood over this infinitesimal time period is then</p> <pre>\\[p^{(1-\\delta)}(1-p)^\\delta\\approx\\exp\\Big\\{\\!-\\!(1-\\delta)\\mu(i,t)\\text{d}t\\Big\\} \\Big\\{\\mu(i,t)\\text{d}t\\Big\\}^\\delta\\]</pre> <p>and hence the log-likelihood is</p> <pre>\\[\\delta\\log\\Big\\{\\mu(i,t)\\text{d}t\\Big\\} - (1-\\delta)\\mu(i,t)\\text{d}t\\]</pre> <p>How can we take the log of the infinitesimal \\(\\text{d}t\\)? The answer is that, even though they are often not presented this way, log-likelihoods are always relative, i.e. it is only the difference between two log-likelihoods that matters. And, when we subtract another log-likelihood for the same experience data<sup>2</sup>, the \\(\\text{d}t\\)s inside the log term will cancel.</p> <p>Given our assumption of independence by individual and time period, we can sum these individual log-likelihoods. When we do this, \\(\\delta\\) is, by definition zero everywhere except the very end and so the \\((1-\\delta)\\) can be treated as \\(1\\), which leaves us with<sup>3</sup></p> <pre>\\[\\delta\\log \\mu(i,t) - \\mu(i,t)\\text{d}t\\tag{3}\\]</pre> <p>But we already know how to add up these terms using the \\(\\text{A}\\) and \\(\\text{E}\\) operators defined in the first article in this series. So the log-likelihood is</p> <pre>\\[L=\\text{A}w\\log\\mu-\\text{E}w\\tag{4}\\]</pre> <p>where \\(w\\) is a variable weighting of the log-likelihood infinitesimals.</p> <p>\\(\\text{A}\\) and \\(\\text{E}\\) are measures, so \\(L\\) is a measure<sup>4</sup> too, which means that, just like \\(\\text{A}\\) and \\(\\text{E}\\), we can partition the data any way we like \u2013 we\u2019ll still end up the same result (Insight\u00a02).</p>"},{"location":"2025-08/mortality-log-likelihood/#weighting","title":"Weighting","text":"<p>Log-likelihood is literally the log of a probability so the inclusion of a weight \\(w\\) implies that we\u2019re using probabilities to the power of \\(w\\), which is worthy of comment.</p> <ol> <li> <p>If \\(w\\in\\{0,1\\}\\) then this is simply equivalent to excluding or including data. I\u2019ll call this \u2018lives-weighted\u2019.</p> </li> <li> <p>If \\(0\\le w \\le 1\\) then \\(w\\) can be interpreted as relevance (also known as \u2018reliability\u2019 or \u2018importance\u2019).</p> </li> <li> <p>The general case, \\(w\\ge0\\), is sometimes described as \u2018ad hoc\u2019 or \u2018pragmatic\u2019, or even illegitimate(!).</p> </li> </ol> <p>I\u2019ll have more to say about weighting the log-likelihood in due course. For the time being, let\u2019s leave our options open by assuming \\(w\\ge0\\), i.e. 3 above.</p> <p>Insight 7. Log-likelihood can be defined directly in terms of the \\(\\text{A}\\) and \\(\\text{E}\\) operators</p> <p>The log-likelihood written in terms of the \\(\\text{A}\\) and \\(\\text{E}\\) operators is</p> <pre>\\[L=\\text{A}w\\log\\mu-\\text{E}w\\]</pre> <p>where \\(w\\ge0\\) is the weight variable.</p> <p>(This is before allowing for overdispersion.)</p> <p>[All mortality insights]</p> <p>Next article: Proportional hazards</p> <p>Equation \\((4)\\) is doing a lot of heavy lifting with admirable concision. In the next article, I\u2019ll show how it leads directly to one of the most useful tools in the mortality modelling armoury.</p> <ol> <li> <p>My mental model of log-likelihood is as a sum of infinitesimals. It is all heuristics though \u2013 I am not making a pretence of mathematical rigour.\u00a0\u21a9</p> </li> <li> <p>A cardinal rule of log-likelihoods is that they are comparable only if calculated on exactly the same data.\u00a0\u21a9</p> </li> <li> <p>Taking the log of the dimensional quantity \\(\\mu\\) may also irk you \u2013 it does me (a little) \u2013 but units also cancel when two log-likelihoods are compared.\u00a0\u21a9</p> </li> <li> <p>Technically it\u2019s a signed measure.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-08/mortality-proportional-hazards/","title":"Mortality: Proportional hazards","text":"<p>A/E diagnostics are important but, if we have any mortality experience data, we should be using it to develop a model that takes account of that data, even if it\u2019s nothing more than a simple how-much-heavier-or-lighter-is-the-mortality-of-this-population-than-average model. Otherwise, we\u2019re not making full use of available information.</p> <p>There are lots of possible approaches, including complex parametric formulas designed to capture all typically observed effects. But I promised concision and so in this article I\u2019ll expound what I think is simultaneously one of the most powerful general approaches and one of the simplest. And the beauty of it is: we\u2019ve already done most of the work.</p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol> <p>To make this concrete, let\u2019s assume that we have a mortality model \\(\\mu(\\beta)\\), where \\(\\beta\\) is a vector of real parameters<sup>1</sup> and that \\(\\mu(\\beta)\\) is differentiable by \\(\\beta\\).</p> <p>A standard way to estimate \\(\\beta\\) is to choose the value that maximises the log-likelihood, i.e.</p> <pre>\\[\\hat\\beta = \\underset{\\beta}{\\arg\\max}\\, L(\\beta)\\tag{5}\\]</pre> <p>where \\(L\\) is the log-likelihood and the parameter \\(\\beta\\) is passed through to the mortality \\(\\mu(\\beta)\\).</p> <p>We saw in the previous article that the log-likelihood is</p> <pre>\\[L=\\text{A}w\\log\\mu-\\text{E}w\\tag{4}\\]</pre> <p>where \\(w\\ge0\\) is an optional weighting variable.</p> <p>We can try solving equation \\((5)\\) by setting the derivative of \\(L\\) to zero, resulting in the vector differential equation</p> <pre>\\[\\frac{\\partial L}{\\partial\\beta}(\\hat\\beta) = 0\\tag{6}\\]</pre> <p>Recalling that there is an implicit \\(\\mu(\\beta)\\) in the \\(\\text{E}\\) operator, we can re-express the derivative as</p> <pre>\\[\\frac{\\partial L}{\\partial\\beta}=\\text{A}wX-\\text{E}wX\\tag{7}\\]</pre> <p>where</p> <pre>\\[X=\\frac{1}{\\mu}\\frac{\\partial \\mu}{\\partial\\beta}\\tag{8}\\]</pre>"},{"location":"2025-08/mortality-proportional-hazards/#the-proportional-hazards-model","title":"The proportional hazards model","text":"<p>The form of equation \\((7)\\) begs the question: what if the vector \\(X\\) were an object in its own right, independent of \\(\\beta\\), i.e. \\(\\partial X/ \\partial\\beta=0\\)? </p> <p>Equation \\((8)\\) would then be a simple first order differential equation in \\(\\mu\\), for which we can write down the solution as</p> <pre>\\[\\mu(\\beta) = \\mu^\\text{ref}\\exp\\Big(\\beta^\\text{T}X\\Big)\\tag{9}\\]</pre> <p>where</p> <ul> <li>\\(\\mu^\\text{ref}\\) is a mortality that does not depend on \\(\\beta\\), and</li> <li>\\(\\beta^\\text{T}X\\) means the \u2018dot\u2019 product of vectors \\(\\beta\\) and \\(X\\), i.e. \\(\\sum_j \\beta_j X_j\\),</li> </ul> <p>and don\u2019t forget that \\(\\mu\\), \\(\\mu^\\text{ref}\\) and the components of the covariate vector \\(X\\) are all variables and therefore also have implicit fact (\\(i\\)) and time (\\(t\\)) arguments.</p> <p>Equation \\((9)\\) is the well-known proportional hazards model<sup>2</sup>, with the elements of \\(X\\) being the covariates and the elements of \\(\\beta\\) the fitted covariate weights.</p> <p>Some observations:</p> <ul> <li> <p>Let\u2019s first note that we were led to equation \\((9)\\) simply by writing the log-likelihood in terms of the \\(\\text{A}\\) and \\(\\text{E}\\) operators and the symmetry of equation \\((7)\\).</p> </li> <li> <p>The name \u2018proportional hazards\u2019 is not ideal because hazard<sup>3</sup> rates combine using addition, i.e. \\(\\mu=\\sum_i\\mu^{(i)}\\), not multiplication, i.e. \\(\\mu=\\prod_i e^{\\beta_iX_i}\\). So the covariates do not relate to individual hazard rates; instead they are component effects used to build a model. The ultimate justification is the effectiveness \u2013 power and tractability \u2013 of the proportional hazards approach.</p> </li> <li> <p>Finally, this is part of a bigger picture in which linear \\(\\log\\mu\\)<sup>4</sup> models are ubiquitous, from Gompertz, arguably the world\u2019s first realistic mortality model, via Lee-Carter<sup>5</sup> to the CMI Mortality Projections Model.</p> </li> </ul> <p>We still need to solve equation \\((6)\\), which, in general, we have to do numerically. The good news is that, provided we use a proportional hazards model, we can write down the first and second derivative of the log-likelihood in closed form. That in turn means we can use Newton\u2013Raphson, which, in my experience is robust<sup>6</sup> and beats most other numerical methods hands down<sup>7</sup>. The vector first derivative (from equation \\((7)\\) above) and the matrix second derivative are</p> <pre>\\[\\begin{aligned}\nL'&amp;=\\text{A}wX-\\text{E}wX\n\\\\[1em]\nL''&amp;=-\\text{E}wXX^\\text{T}\n\\end{aligned}\\]</pre> <p>where I have used \\('\\) to indicate \\(\\partial/\\partial\\beta\\).</p> <p>At risk of repetition, note (a)\u00a0the concision and (b)\u00a0that everything can be expressed in terms of \\(\\text{A}\\) and \\(\\text{E}\\), which means we\u2019re re-using  existing machinery for these calculations.</p>"},{"location":"2025-08/mortality-proportional-hazards/#information-budget","title":"Information budget","text":"<p>Many expositions of the proportional hazards model do not include \\(\\mu^\\text{ref}\\), i.e. a given \u2018background\u2019 hazard rate, but for mortality analysis this is often optimal. For instance:</p> <ul> <li> <p>If you\u2019re analysing DB pension plan mortality experience over the last ten years, you probably don\u2019t want to be trying to calibrate a mortality trends model<sup>8</sup> at the same time, which you can avoid by putting your pre-existing mortality trends assumption into \\(\\mu^\\text{ref}\\).</p> </li> <li> <p>I\u2019d suggest going further and modelling variation from a reasonable default mortality (including trends) so that you inherit a priori sensible behaviour<sup>9</sup> from that default.</p> </li> <li> <p>At the most extreme, if you have a postcode mortality model to hand, then use that as \\(\\mu^\\text{ref}\\)<sup>10</sup>.</p> </li> </ul> <p>In general, you want to spend the information budget provided by the experience data on fitting the unknowns you don\u2019t know as opposed to spending it on refitting things you likely already do. The proportional hazards model makes this straightforward.</p>"},{"location":"2025-08/mortality-proportional-hazards/#one-model-to-rule-them-all","title":"One model to rule them all","text":"<p>In my experience, the proportional hazards model is all you need in practice. The richness available from the infinite range of possible covariates, the sheer tractability of the approach, the straightforwardness of using a prior model and the interpretability of the results provide enough firepower to tackle any real world mortality modelling problem.</p> <p>Insight 8. Proportional hazards models are probably all you need for mortality modelling</p> <p>The proportional hazards model</p> <pre>\\[\\mu(\\beta) = \\mu^\\text{ref}\\exp\\Big(\\beta^\\text{T}X\\Big)\\]</pre> <p>is</p> <ul> <li>highly tractable, and</li> <li>sufficiently powerful to cope with almost all practical mortality modelling problems.</li> </ul> <p>[All mortality insights]</p> <p>There is a lot more to this, e.g. how does mortality vary between populations, do we require additional procedures to select covariates initially, and so on, which are questions I hope to answer in due course.</p> <p>But, for now, let\u2019s take stock:</p> <ul> <li>With the proportional hazards model, we have an excellent framework for creating mortality models.</li> <li>And, by maximising log-likelihood, we can calibrate those models with relative ease.</li> </ul> <p>Next article: Suddenly AIC</p> <p>The obvious next question is: how should we choose between different models? This will be the subject of the next article.</p> <ol> <li> <p>Recall that, as a mortality, \\(\\mu(\\beta)\\) also has implicit fact (\\(i\\)) and time (\\(t\\)) arguments, but these are not shown to ease the notational burden.\u00a0\u21a9</p> </li> <li> <p>It is sometimes called the \u2018Cox proportional hazards model\u2019, but this is usually in a context (e.g. medical research) where the objective is to estimate an impact independently from the absolute mortality (or \u2018hazard\u2019) rate, which is the opposite of what we\u2019re doing here \u2013 we are trying to calculate the absolute hazard rate.\u00a0\u21a9</p> </li> <li> <p>\u2018Hazard\u2019 is the general term for what actuaries often call a \u2018decrement\u2019.\u00a0\u21a9</p> </li> <li> <p>When you think about it, it is a little odd that \\(\\log\\mu\\) should be the natural metric when \\(\\mu\\) itself is already the log of something. There is a very good explanation for this, but that\u2019s to come.\u00a0\u21a9</p> </li> <li> <p>OK, Lee-Carter is bi-linear.\u00a0\u21a9</p> </li> <li> <p>Some initial damping may be required.\u00a0\u21a9</p> </li> <li> <p>First, it converges quadratically, and, second, the cost of inverting \\(L''\\) (sometimes cited as a reason not to use Newton-Raphson) is typically very small compared with the cost of calculating \\(L'\\) and \\(L''\\). Failure to converge is often because the model being calibrated has an identifiability issue, resulting in \\(L''\\) being non-invertible, which in itself is a handy diagnostic.\u00a0\u21a9</p> </li> <li> <p>Mortality trends are hard to discern over a period as short as ten years, and breaking them down into age, period and cohort components is even harder. The CMI uses 41 years for its Mortality Projections Model and it still struggles. Trends also typically include some element of judgement or consensus view, to which you likely want to adhere in your general mortality modelling.\u00a0\u21a9</p> </li> <li> <p>An important example is high age mortality, which is nigh impossible  to determine from an individual pension plan\u2019s experience data. By the very nature of high ages, there is little data and that data is often unreliable \u2013 even if the plan is huge, systemic data risk remains. So, in practice, you will need some sort of prior model for high age mortality.</p> <p>This is one of the reasons I set up the CMI\u2019s High Age Mortality Working Party. The high quality \u2013 and prize-winning \u2013 reports (WP100 and WP122) by Steve Bale and his working party colleagues are recommended reading.</p> <p>Incidentally, \u2018high age\u2019 in a pension plan longevity context typically means over age 90 or so.\u00a0\u21a9</p> </li> <li> <p>In this case, you\u2019ll also need a means of determining the weight to place on the experience compared with your postcode model, but that\u2019s a separate subject.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-08/mortality-suddenly-aic/","title":"Mortality: Suddenly AIC","text":"<p>In this article, I\u2019m going to look at choosing between mortality models using Hirotugu Akaike\u2019s information criterion, the AIC.</p> <p>I\u2019m going to run through \u2013 at a very high level \u2013 the rationale behind the AIC and its construction because</p> <ul> <li>the standard result looks so trivial that people sometimes assumes it\u2019s an arbitrary convention, and</li> <li>I\u2019m going to generalise it (a little).</li> </ul> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol> <p>I previously wrote that it\u2019s a shame that the \u2018log\u2019 in \u2018log-likelihood\u2019 is not always recognised as being much more fundamental than a mere technical convenience or device for avoiding numerical under/overflow. The AIC is a case in point \u2013 indeed Akaike himself wrote that \u2018the log-likelihood is essentially a more natural quantity than the simple likelihood\u2019<sup>2</sup>.</p>"},{"location":"2025-08/mortality-suddenly-aic/#akaikes-big-idea","title":"Akaike\u2019s big idea","text":"<p>Selecting parameters by maximising the log-likelihood is an effective tool for calibrating a single model. But we can\u2019t choose between different models by comparing maximum likelihoods because models with more parameters can achieve a better fit to the data and so the comparison is not fair.</p> <p>Further reading</p> <p>Kenneth Burnham and David Anderson\u2019s book on model selection<sup>1</sup> is in my view the best reference on practical application of the AIC. Also see (both online) AIC vs BIC and AIC myths and misunderstandings.</p> <p>Rob Hyndman provides an excellent overview of facts and fallacies of the AIC.</p> <p>Akaike\u2019s<sup>2</sup> insight was</p> <ul> <li> <p>relative entropy (also known as the Kullback-Leibler divergence) can be used to compare different models,</p> </li> <li> <p>maximum likelihood is a biased estimate of the variation in relative entropy compared with reality<sup>3</sup>,</p> </li> <li> <p>provided a model is reasonably good, we can adjust for that bias with a simple penalty, and</p> </li> <li> <p>the resulting penalised maximum log-likelihood, \\(L_\\text{P}\\), can be used to compare models regardless of the number of parameters they have.</p> </li> </ul> <p>Too many twos</p> <p>The AIC is defined as \u22122 times \\(L_\\text{P}\\), for consistency with regression.</p> <p>Unfortunately the \u22122 factor</p> <ul> <li>complicates the definition,</li> <li>obscures the AIC as an unbiased estimate of relative entropy, and</li> <li>leads to spurious additional factors of 2 or \u00bd in applications<sup>4</sup>.</li> </ul> <p>So I\u2019ll use the penalised log-likelihood, \\(L_\\text{P}\\), itself i.e. without the \u22122.</p>"},{"location":"2025-08/mortality-suddenly-aic/#penalised-log-likelihood","title":"Penalised log-likelihood","text":"<p>Although I\u2019ll continue to refer to \u2018the AIC\u2019, I\u2019ll express it in terms of penalised log-likelihood, which is equivalent but cleaner (see box out).</p> <p>I\u2019m not going to justify or derive the AIC here \u2013 if you want further information then see Burnham &amp; Anderson<sup>1</sup> \u2013 but we will need a few key results.</p> <p>The penalised log-likelihood that falls out of the definition of relative entropy is</p> <pre>\\[L_\\text{P} \\mathrel{\\hat=} \\mathbb{E}_1\\mathbb{E}_2\\,L\\tag{10}\\]</pre> <p>where</p> <ul> <li> <p>\\(\\mathbb{E}\\) indicates expectation according to reality, which occurs once in the definition of relative entropy itself and a second time because we want our estimate to be unbiased, and</p> </li> <li> <p>\\(L\\) is log-likelihood (weighted by the variable \\(w\\ge0\\)), which we previously defined  using the \\(\\text{A}\\) and \\(\\text{E}\\) operators as</p> <pre>\\[L=\\text{A}w\\log\\mu-\\text{E}w\\tag{4}\\]</pre> </li> </ul> <p>With some assumptions, we can estimate this as</p> <pre>\\[L_\\text{P} = L(\\hat\\beta)-\\operatorname{tr}\\Big\\{\\mathbf{I} \\, \\text{Var}\\big(\\hat\\beta\\big)\\Big\\}\\tag{11}\\]</pre> <p>where \\(\\beta\\) parametrises the mortality model under consideration, \\(\\hat\\beta\\) maximises the log-likelihood, and all of the following are \\(\\dim(\\beta)^2\\) square matrices:</p> <pre>\\[\\begin{aligned}\n\\text{Var}\\big(\\hat\\beta\\big)&amp;\\approx\\mathbf{I}^{-1}\\mathbf{J}\\mathbf{I}^{-1}\n\\\\[0.5em]\n\\mathbf{I}&amp;=-L''(\\hat\\beta)\n\\\\[0.5em]\n\\mathbf{J}&amp;\\mathrel{\\hat=}\\mathbb{E}\\,L'(\\beta)L'(\\beta)^\\text{T}\n\\end{aligned}\\]</pre> <p>with \\('\\) indicating \\(\\partial /\\partial \\beta\\) and \\(^\\text{T}\\) indicating transpose.</p>"},{"location":"2025-08/mortality-suddenly-aic/#the-pay-off","title":"The pay off","text":"<p>The \\(\\mathbf{I}\\) matrix and its inverse cancel in equation \\((11)\\), resulting in</p> <pre>\\[L_\\text{P}=L(\\hat\\beta)-\\text{tr}(\\mathbf{J}\\mathbf{I}^{-1})\\tag{12}\\]</pre> <p>where \\(\\text{tr}\\) is the trace operator.</p> <p>The usual next step is to note that, in the lives-weighted case, i.e. \\(w\\in\\{0,1\\}\\), \\(\\mathbf{I}\\approx\\mathbf{J}\\), which results in the conventional form<sup>5</sup> of the AIC:</p> <pre>\\[L_\\text{P}=L(\\hat\\beta)-\\dim(\\beta)\\tag{lives-weighted only}\\]</pre> <p>But we can still obtain a useful result in the ad hoc weighted case if we assume that our mortality model is proportional hazards, i.e.</p> <pre>\\[\\mu(\\beta) = \\mu^\\text{ref}\\exp\\Big(\\beta^\\text{T}X\\Big)\\tag{9}\\]</pre> <p>From before, we have</p> <pre>\\[\\mathbf{I}=-L''(\\hat\\beta)=\\text{E}wXX^\\text{T}\\]</pre> <p>and, using the expected and variance results from the A over E article, we have</p> <pre>\\[\\begin{aligned}\n\\mathbb{E}\\,L'(\\beta)L'(\\beta)^\\text{T}\n&amp;=\\mathbb{E}\\big(\\text{A}wX-\\text{E}wX\\big)\\big(\\text{A}wX-\\text{E}wX\\big)^\\text{T}\n\\\\[0.6em]\n&amp;\\approx\\text{E}w^2XX^\\text{T}\n\\end{aligned}\\]</pre> <p>resulting in our estimator for \\(\\mathbb{E}\\,L'(\\beta)L'(\\beta)^\\text{T}\\) being</p> <pre>\\[\\mathbf{J}=\\text{E}w^2XX^\\text{T}\\tag{13}\\]</pre> <p>and hence</p> <pre>\\[\\text{Var}\\big(\\hat\\beta\\big)\\mathrel{\\hat=} \\big(\\text{E}wXX^\\text{T}\\big)^{-1}\\big(\\text{E}w^2XX^\\text{T}\\big)\\big(\\text{E}wXX^\\text{T}\\big)^{-1}\\tag{14}\\]</pre> <p>and</p> <pre>\\[L_\\text{P}=L(\\hat\\beta)-\\text{tr}\\left(\\frac{\\text{E}w^2XX^\\text{T}}{\\text{E}wXX^\\text{T}}\\right)\\tag{15}\\]</pre> <p>Insight 9. An estimate of the variance of the fitted parameters for a proportional hazards mortality model is available in closed form for any ad hoc log-likelihood weight</p> <pre>\\[\\text{Var}\\big(\\hat\\beta\\big)\\mathrel{\\hat=} \\mathbf{I}^{-1}\\mathbf{J}\\mathbf{I}^{-1}\\]</pre> <p>where \\(\\hat\\beta\\) is the maximum likelihood estimator of the covariate weights, \\(X\\) is the vector of covariates, \\(w\\ge0\\) is the log-likelihood weight, \\(\\mathbf{I}=\\text{E}wXX^\\text{T}\\) and \\(\\mathbf{J}=\\text{E}w^2XX^\\text{T}\\).</p> <p>(This is before allowing for overdispersion.)</p> <p>Caveat: \\(w\\) is an ad hoc reallocation of log-likelihood; it is not relevance. For the version of this insight that does take account of relevance, see Insight\u00a017.</p> <p>[All mortality insights]</p> <p>Insight 10. A penalised log-likelihood for a proportional hazards mortality model is available in closed form for any ad hoc log-likelihood weight</p> <pre>\\[L_\\text{P}= L(\\hat\\beta)-\\text{tr}\\big(\\mathbf{J}\\mathbf{I}^{-1}\\big)\\]</pre> <p>where \\(\\hat\\beta\\) is the maximum likelihood estimator of the covariate weights, \\(X\\) is the vector of covariates, \\(L\\) is the log-likelihood, \\(w\\ge0\\) is the log-likelihood weight, \\(\\mathbf{I}=\\text{E}wXX^\\text{T}\\) and \\(\\mathbf{J}=\\text{E}w^2XX^\\text{T}\\).</p> <p>Caveat: \\(w\\) is an ad hoc reallocation of log-likelihood; it is not relevance. For the version of this insight that does take account of relevance, see Insight\u00a017.</p> <p>[All mortality insights]</p>"},{"location":"2025-08/mortality-suddenly-aic/#just-weight-a-moment","title":"Just weight a moment","text":"<p>Provided we use a proportional hazards model, we have formulas for penalised log-likelihood and the variance of \\(\\hat\\beta\\) in the pragmatic ad hoc weighted case.</p> <p>In the lives-weighted case, \\(w\\in\\{0,1\\}\\), then \\(w^2=w\\) and hence equation \\((15)\\) collapses to the lives-weighted version. Otherwise, weighting the experience data will increase concentration and so the estimated variance of \\(\\hat\\beta\\) will typically be greater than the lives-weighted version. If that reflects the impact of \\(\\hat\\beta\\) on the liabilities then this is intuitively reasonable.</p> <p>There are issues though:</p> <ul> <li> <p>The validity of equation \\((13)\\) depends on the interpretation of the weight, \\(w\\). If it is lives-weighted, i.e. \\(w\\in\\{0,1\\}\\), then everything is trivially ok. And if \\(w\\) is an ad hoc reallocation of log-likelihood then this seems ok too, albeit in a pragmatic hand-wavy sense<sup>6</sup>. But equation \\((13)\\) cannot be correct if \\(w\\) is interpreted as, for instance, relevance \u2013 if it were then, say, halving it should double \\(\\text{Var}\\big(\\hat\\beta\\big)\\), but it doesn\u2019t. Hence the caveats.</p> </li> <li> <p>Being able to rank models by \\(L_\\text{P}\\) gets us only halfway because we also need to understand whether differences in \\(L_\\text{P}\\) are significant. In the lives-weighted and (after we\u2019ve corrected for the point above) the relevance-weighted cases, this is straightforward \u2013 a difference in the penalised log-likelihood of 1 is significant because it is equivalent to adding one more parameter. But, for an ad hoc weight, using the formula as presented here does not automatically provide a measure of significance and so we need another way in<sup>7</sup>.</p> </li> </ul> <p>On balance, I think equation \\((15)\\) is a useful practical tool, but we can do better and put matters on a sounder footing. So I\u2019ll revisit weighting and relevance in a few articles\u2019 time.</p> <p>Next article: Overdispersion and quasi-log-likelihood</p> <p>I used variance in the above with allowing for overdispersion. This is always wrong in mortality analysis, so let\u2019s tackle it in the next article.</p> <ol> <li> <p>Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel Inference: A practical information-theoretic approach (2nd ed.), Springer-Verlag. doi:10.1007/b97636, ISBN-13: 9780387953649.\u00a0\u21a9\u21a9</p> </li> <li> <p>Akaike, H. (1973), \u201cInformation theory and an extension of the maximum likelihood principle\u201d, in Petrov, B. N.; Cs\u00e1ki, F. (eds.), 2nd International Symposium on Information Theory, Tsahkadsor, Armenia, USSR, September 2-8, 1971, Budapest: Akad\u00e9miai Kiad\u00f3, pp. 267\u2013281. Republished in Kotz, S.; Johnson, N. L., eds. (1992), Breakthroughs in Statistics, vol. I, Springer-Verlag, pp. 610\u2013624.\u00a0\u21a9\u21a9</p> </li> <li> <p>This does not imply that there is a \u2018true model\u2019 \u2013 models are by definition simplifications of reality.\u00a0\u21a9</p> </li> <li> <p>Examples:</p> <ul> <li> <p>The natural definition and standard guidance on what constitutes a significant difference in AIC is one parameter\u2019s worth. This is 1 for penalised log-likelihood, but a non-intuitive 2 for the conventional AIC definition.</p> </li> <li> <p>Akaike weights, \\(\\exp(-\\tfrac{1}{2}\\text{AIC})\\), are pure clumsiness compared with \\(\\exp(L_\\text{P})\\).</p> </li> </ul> <p>\u21a9</p> </li> <li> <p>Barring a factor of \u22122.\u00a0\u21a9</p> </li> <li> <p>There is non-hand-wavy version to come in a few articles\u2019 time.\u00a0\u21a9</p> </li> <li> <p>One option is to add up all the penalties of all candidate models and divide that the by the total number of parameters being fitted to obtain an equivalent to \u2018one more parameter\u2019.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-09/mortality-overdispersion-and-quasi-log-likelihood/","title":"Mortality: Overdispersion and quasi-log-likelihood","text":"<p>In the first article in this series, I noted that if you don\u2019t allow for overdispersion then you will underestimate uncertainty and overfit models.</p> <p>In this article I\u2019ll outline the most practical approach to dealing with overdispersion in a pensions longevity context.</p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol> <p>In the real world, mortality experience data typically exhibits greater variance than would be the case if, as we\u2019ve assumed up to now, that mortality itself is deterministic and deaths are independent. </p> <p>Specifically, if we define overdispersion<sup>1</sup> as</p> <pre>\\[\\Omega=\\frac\n{\\text{Var}\\big(\\text{A}w-\\text{E}w\\big)}\n{\\mathbb{E}\\big(\\text{E}w^2\\big)}\n\\tag{16}\n\\]</pre> <p>then we typically observe \\(\\Omega \\gt 1\\) instead of the \\(\\Omega = 1\\) we\u2019d expect based on our assumptions.</p> <p>Overdispersion tends not to affect the estimated parameters themselves (if you\u2019re using maximum likelihood), but it does affect their estimated variance, potentially leading to over-fitted models being selected if it is not accounted for. So we need to allow for overdispersion in some way.</p>"},{"location":"2025-09/mortality-overdispersion-and-quasi-log-likelihood/#causes-of-overdispersion","title":"Causes of overdispersion","text":"<p>It\u2019s not heterogeneity!</p> <p>At time of writing (2025-08-20), the Wikipedia article on overdispersion states (my italics):</p> <p>Overdispersion is a very common feature in applied data analysis because in practice, populations are frequently heterogeneous (non-uniform) contrary to the assumptions implicit within widely used simple parametric models.</p> <p>This explanation is sometimes repeated in a mortality context, but that\u2019s not the case within the framework I\u2019ve outlined: if mortality itself were deterministic (as we\u2019ve assumed) and deaths were independent then, regardless of how much mortality varied by individual, there would be no overdispersion. So the cause of overdispersion is not heterogeneity; it\u2019s the failure of our idealised assumptions.</p> <p>And it\u2019s not just that heterogeneity is not the cause in this context; mortality varying by individual is a core premise in mortality modelling<sup>2</sup>.</p> <p>There are multiple potential causes of overdispersion, all arising from our initial assumptions not being true in the real world, the key ones being that mortality is itself deterministic and deaths are independent. (Also see the box out regarding heterogeneity.)</p> <p>We knew from the the outset that this is a convenient simplification of reality. The question is: what\u2019s the best way to allow for overdispersion<sup>3</sup>?</p>"},{"location":"2025-09/mortality-overdispersion-and-quasi-log-likelihood/#quasi-log-likelihood","title":"Quasi log-likelihood","text":"<p>One option is to use a statistical distribution with an additional hyperparameter to model observed overdispersion explicitly. People do do this, but it entails a loss of tractability<sup>4</sup> and there is a simpler alternative.</p> <p>The most widely used approach is to divide the log-likelihood by an estimated or assumed global<sup>5</sup> value for overdispersion, \\(\\Omega\\), i.e.</p> <pre>\\[L \\mapsto \\Omega^{-1}L\\]</pre> <p>Technically, this results in a quasi-log-likelihood (and the AIC becomes the QAIC), but I\u2019ll continue to refer to it as the log-likelihood on the understanding that adjustments are always made for overdispersion.</p> <p>The question naturally arises as to whether to estimate overdispersion from the experience data to hand<sup>6</sup>. While measured overdispersion should be a test diagnostic (provided you have sufficient data), there is a case for using a default value:</p> <ul> <li>We have a reasonable prior view on mortality overdispersion \u2013 it is better in general to spend your information budget on estimating the things you don\u2019t know. </li> <li>Estimating \\(\\Omega\\) from the data can add brittleness and hard-to-explain variation to model fitting.</li> <li>Estimating \\(\\Omega\\) from the data also makes the fitting process more complicated, increasing the potential sources of error.</li> </ul> <p>I\u2019d suggest a suitable default is \\(2\\le\\Omega\\le3\\)<sup>7</sup>, with higher values making your models a little more resistant to overfitting.</p> <p>If you do estimate \\(\\Omega\\) from the data then the cardinal rule is use the same estimate for all candidate models being tested.</p> <p>Insight 11. Adjusting globally for overdispersion is reasonable and straightforward</p> <p>If \\(\\Omega\\) is global overdispersion then:</p> <ol> <li> <p>A standard method for allowing for overdispersion is to scale log-likelihood by \\(\\Omega^{-1}\\) and variances by \\(\\Omega\\).</p> </li> <li> <p>Suitable default values for mortality experience data are \\(2\\le\\Omega\\le3\\).</p> </li> <li> <p>Use the same \\(\\Omega\\) for all candidate models being tested, including when \\(\\Omega\\) is being estimated from the experience data at hand.</p> </li> </ol> <p>[All mortality insights]</p>"},{"location":"2025-09/mortality-overdispersion-and-quasi-log-likelihood/#formulas-including-global-overdispersion","title":"Formulas including global overdispersion","text":"<p>For reference, I\u2019ll restate results from previous articles with overdispersion included.</p> <ol> <li> <p>The variance result:</p> <pre>\\[\\text{Var}\\big(\\text{A}w-\\text{E}w\\big)=\\Omega\\,\\mathbb{E}\\big(\\text{E}w^2\\big) \\tag{2b}\\]</pre> </li> <li> <p>Log-likelihood:</p> <pre>\\[L=\\Omega^{-1}\\big(\\text{A}w\\log\\mu-\\text{E}w\\big) \\tag{4b}\\]</pre> </li> <li> <p>The (estimated) variance of the fitted covariate weights \\(\\hat\\beta\\):</p> <pre>\\[\\text{Var}\\big(\\hat\\beta\\big)\\mathrel{\\hat=} \\Omega\\,\\mathbf{I}^{-1}\\mathbf{J}\\mathbf{I}^{-1} \\tag{14b}\\]</pre> </li> <li> <p>The penalised log-likelihood is unchanged provided we use the adjusted (quasi-)log-likelihood from above:</p> <pre>\\[L_\\text{P}=L(\\hat\\beta)- \\text{tr}\\big(\\mathbf{J}\\mathbf{I}^{-1}\\big)\\tag{15b}\\]</pre> </li> </ol> <p>In the above, \\(\\mathbf{I}=\\text{E}wXX^\\text{T}\\) and \\(\\mathbf{J}=\\text{E}w^2XX^\\text{T}\\), and, as previously noted, these assume \\(w\\) is an ad hoc reallocation of log-likelihood, as opposed to e.g. relevance-based.</p> <p>Next article: Incoherent rating factors</p> <p>So far I have focussed on the machinery for modelling the mortality of DB pension plan members. There is more to come on this, but in the next article I want to discuss \u2018incoherent rating factors\u2019. These are rating factors that can appear predictive using standard modelling diagnostics but which \u2013 scarily \u2013 can result in models that produce poor or even systematically biased forecasts.</p> <ol> <li> <p>Equation \\((16)\\) is more general than the usual definition of overdispersion because I have included a weight \\(w\\) so as to ensure that the additional variance of arising from variation in the weight \\(w\\) itself is explicitly excluded.\u00a0\u21a9</p> </li> <li> <p>There are other issues caused by heterogeneity, e.g. it is a candidate for causing the flattening of observed population mortality at high ages and failure to allow for heterogeneity can result in the under-valuation of closed books of pension-in-payment liabilities. I may return to these in future articles.\u00a0\u21a9</p> </li> <li> <p>Duplicates in the data could be considered an extreme version of deaths not being independent, but I\u2019ll assume that every attempt has been made in practice to deduplicate the data.\u00a0\u21a9</p> </li> <li> <p>Tractability doesn\u2019t just mean pretty equations; it means simpler computer code that is therefore more robust and easier to maintain, which in turn means fewer visits from the f***-up fairy.\u00a0\u21a9</p> </li> <li> <p>It is not necessarily the case that overdispersion is the same for all the data in the experience dataset, or indeed for all weights in equation \\((16)\\), but this is a standard assumption on the basis that, once a general mitigation has been made for overdispersion, then there are probably bigger modelling fish left to fry.\u00a0\u21a9</p> </li> <li> <p>I think the original CMI Mortality Projections Model used to do this. It doesn\u2019t any more, which means overdispersion is subsumed into its various \\(S\\) parameters.\u00a0\u21a9</p> </li> <li> <p>Burnham &amp; Anderson recommend \\(1\\le\\Omega\\le4\\) for general count data. See box out \u2018Overdispersed Count Data: A Review\u2019 on page 69 of Burnham, K. P.; Anderson, D. R. (2002), Model Selection and Multimodel Inference: A practical information-theoretic approach (2nd ed.), Springer-Verlag. doi:10.1007/b97636, ISBN-13: 9780387953649.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-10/mortality-incoherent-rating-factors/","title":"Mortality: Incoherent rating factors","text":"<p>A pre-requisite for pension plan mortality modelling is that the rating factors used should be coherent, which I\u2019ll define shortly.</p> <p>The insidious problem with incoherent rating factors is that a model may produce poor or even systemically biased forecasts while simultaneously performing well on standard model fit and selection diagnostics. Yikes!</p> <p>For large experience datasets in particular, rating factor incoherence can be a bigger issue than model fitting and selection. But too often in practice, I\u2019ve seen this issue trivialised or deemed \u2018obvious\u2019.</p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol>"},{"location":"2025-10/mortality-incoherent-rating-factors/#defining-rating-factor-coherence","title":"Defining rating factor coherence","text":"<p>A rating factor is a function of the facts available for each individual that can validly be used as a model covariate or weight. I suggest that all of the following<sup>1</sup> must hold in order for this to be the case:</p> <ol> <li>No foreknowledge of death</li> <li>Correspondence between exits and survivors</li> <li>Comparability between individuals</li> <li>Comparability by time</li> </ol> <p>which I\u2019ll call \u2018coherence\u2019.</p> <p>If you use an incoherent rating factor then you can apparently successfully crank the handle on your modelling machinery but your forecasts may be invalid. And the really scary thing is that most of the rating factors used in pension plan mortality analysis fail at least one of the above<sup>2</sup>.</p> <p>Insight 12. Rating factors must be coherent</p> <p>In order for a function of information associated with individuals to be valid as a rating factor, it must be coherent, which means:</p> <ol> <li>No foreknowledge of death</li> <li>Correspondence between exits and survivors</li> <li>Comparability between individuals</li> <li>Comparability by time</li> </ol> <p>[All mortality insights]</p> <p>The practical questions are:</p> <ul> <li>How might the available rating factors be incoherent and how can we check this?</li> <li>What can we do about it?</li> </ul> <p>To illustrate the issues, I\u2019m going to run through each of the above required properties with examples. </p>"},{"location":"2025-10/mortality-incoherent-rating-factors/#1-no-foreknowledge-of-death","title":"1. No foreknowledge of death","text":"<p>It is statistics 101 that you cannot use actual outcomes as model inputs. Otherwise you end up with models that perform well when tested against historical data, but this is only because they have been provided with information about the future, i.e. they are cheating.</p> <p>This is not always obvious. Take, for example, the use of health-related socio-economic typing of postcodes. Using a health-related measure sounds like exactly the sort of thing you\u2019d want to incorporate in a mortality model. The problem is that some of these incorporate historical mortality experience<sup>3</sup>, which is what mortality models are trying to predict. My recommendations here are:</p> <ul> <li>Check the construction of candidate socio-economic typings to ensure they do not depend on mortality data and exclude those that do.</li> <li>Consider diversifying the socio-economic typings used so as to spread the risk.</li> </ul>"},{"location":"2025-10/mortality-incoherent-rating-factors/#2-correspondence-between-exits-and-survivors","title":"2. Correspondence between exits and survivors","text":"<p>At its core, all mortality analysis compares deaths to E2R, and so inconsistency between rating factors between exits and survivors may result in flawed or even biased models. Ensuring that information for exits and survivors is consistent is generally known as the principle of correspondence.</p> <p>A classic example in relation to UK pension plans is individual pension, which is often provided as at the end of the experience period for survivors but as at date of exit for deaths and other exits<sup>3</sup>. This means pensions for exits need to be adjusted (\u2018revalued\u2019 in UK parlance). This is both finicky \u2013 as anyone who\u2019s had to code up UK pension increases can attest \u2013 but, at the same time, material. Standard checks are:</p> <ul> <li>Compare A/E by time period weighted by lives versus amounts.</li> <li>Compare assumed total pension in each year with the pension payments disclosed in the plan\u2019s annual accounts.</li> </ul>"},{"location":"2025-10/mortality-incoherent-rating-factors/#3-comparability-between-individuals","title":"3. Comparability between individuals","text":"<p>DB pension plan pensions in payment are a function of historical benefit design and employment turnover, as well as pay history. For instance, a generous pension plan for low pay / low turnover employees may result in similar pensions to an ungenerous pension plan for high pay / high turnover employees. This means that it is dangerous to use pension as a rating factor across pension plans<sup>4</sup>, e.g. as part of a general pension plan mortality model. Addressing this is hard (because pension is such a powerful predictor):</p> <ul> <li>Try using other covariates, e.g. derived from postcode, to displace pension.</li> <li>Try using a better proxy for pay or, if available, pay itself (although these come with their own problems).</li> <li>Assess model performance using leave-one out cross validation (or some variation thereof) that reflects how the model will be used in practice<sup>5</sup>.</li> </ul> <p>Don\u2019t forget that pension plans are biased subsets of all pension plan data and pricing is typically per pension plan. This means that assuming pensions in different plans are comparable leaves you exposed to the winner\u2019s curse, i.e. you win precisely the business that your model misprices against you.</p>"},{"location":"2025-10/mortality-incoherent-rating-factors/#4-comparability-by-time","title":"4. Comparability by time","text":"<p>The point of mortality modelling in an actuarial context is to value future cashflows. This means rating factors must be comparable across the historical time period against which the model is fitted and into the future.</p> <p>For instance, in the simplistic mortality model<sup>6</sup></p> <pre>\\[\\mu_{it}=\\mu_{it}^\\text{ref} \\exp\\big(\\beta X_i\\big)\\]</pre> <p>there is an implicit assumption that \\(X_i\\) has the same impact on mortality at all times in the past and in the future.</p> <p>Here are some of the many ways in which this can go wrong:</p> <ul> <li> <p>Models usually (implicitly) assume that the socio-economic factors derived from postcodes do not vary with age, and yet we all know they do: the young are typically cash-constrained, people tend to move up the \u2018property ladder\u2019 over their careers, and postcodes for the elderly may actually be nursing homes. In my experience, handling this issue is a combination of reviewing research to confirm that there is some stability to socio-economic type when moving home and getting comfortable that the liabilities for the young and old are not the material ones.</p> </li> <li> <p>It is tempting to incorporate ill health retirement indicators when analysing pensions in payment (if they are available), but, for many pension plans, the criteria for ill health retirement have been tightened up over time. This means, it may show up as predictive (in very large plans with sufficient data) but the time-based variation of the criteria may bias those predictions. I suggest testing including ill health indicators to see if the results are materially different, but regarding the model without ill health indicators as the robust one.</p> </li> <li> <p>The most common scenario of all for using mortality models is to value all of a pension plan\u2019s liabilities. But if pension is being used as a covariate then what should be used for current actives? It feels like at least some adjustment should be made to allow for potential future accrual to make them comparable to those of retirees. I don\u2019t think there is a single robust approach to this if you are using pension as a covariate.</p> </li> <li> <p>When trying to make pensions comparable over time, we need to consider what a pension represents. Is it (a)\u00a0a proxy for pay before retirement (and therefore correlated with other forms of wealth) or (b)\u00a0a proxy for total income during retirement? In the UK, this issue is usually elided because UK DB pensions are predominantly index-linked both before and after retirement. But in a pension systems where indexation of pensions in payment is less common, e.g. the USA, this may require attention. For instance, all else being equal, under interpretation\u00a0(a), a pension of \u00a310\u202f000\u00a0pa for a 75-year old that hasn\u2019t been increased since retirement implies a relatively more wealthy individual than the same pension for a 65-year old, whereas under interpretation\u00a0(b) no adjustment is required. There is no easy-to-determine right answer to this one.</p> </li> </ul>"},{"location":"2025-10/mortality-incoherent-rating-factors/#pension-as-a-rating-factor","title":"Pension as a rating factor","text":"<p>If there\u2019s a recurring theme in the above then it\u2019s this: beware of using pension as a rating factor. It will almost certainly show up as strongly predictive (regardless of whether or not you\u2019ve allowed for the above issues correctly) but it can catch you out.</p> <p>Insight 13. Take care when using pension as a rating factor</p> <p>Be wary of phrases like \u2018just use pension as a covariate\u2019 because it trivialises the problems involved in making pension a coherent rating factor:</p> <ul> <li>Pensions for individuals in different pension plans are not directly comparable. For general pension plan mortality models consider using leave-one out cross validation to understand this risk and/or using an alternative approach.</li> <li>Pensions as at date of exit need careful adjustment to be consistent with pensions of survivors (which can be non trivial for UK DB plans).</li> <li>Pensions for actives require additional consideration in relation to potential future accrual.</li> <li>Consideration needs to be given to whether or how to adjust pensions for inflation (typically since retirement). This is more of an issue in pension systems where indexation of pensions in payment is less common (e.g. the USA).</li> <li>Do not assume that longevity always increases with benefit amount.</li> </ul> <p>[All mortality insights]</p> <p>Next article: How mortality varies in pension plans</p> <p>In the next article I\u2019ll cover, at a high level, how mortality varies by age for DB pensioners.</p> <p>With that, I\u2019ll have all the ground work in place for the final (three parter) article in this series, which I\u2019m thinking of calling \u2018Good things come to those who weight\u2019. I hope you\u2019ll find it interesting.</p> <ol> <li> <p>In theory, no foreknowledge of death and correspondence are excluded by the definition of fact. In practice, it\u2019s up to you to ensure this is actually the case.\u00a0\u21a9</p> </li> <li> <p>The rating factors typically available for pension plan mortality analysis are birth date, sex/gender, member category (e.g. retiree, dependant in payment, deferred or active), pension and postcode, with postcode typically being used in conjunction with third party directories of socio-economic type and other information.  You might think that at least birth date and sex/gender are reliable, but I\u2019ve seen even these get mangled in practice (including on very large transactions).\u00a0\u21a9</p> </li> <li> <p>This makes sense for the primary users. For health-related postcode mapping, these are organisations that want to understand the current state of affairs. For individual pension plan data, the primary objective is to track and administer benefits correctly.\u00a0\u21a9\u21a9</p> </li> <li> <p>Historical company mergers or switches from DB to DC may mean that pensions are not comparable even within the same plan.\u00a0\u21a9</p> </li> <li> <p>At a minimum, this means testing your model\u2019s performance on predicting the mortality for each plan p in your dataset when the model has been calibrated to data excluding plan p.\u00a0\u21a9</p> </li> <li> <p>This model is deliberately simplified. For instance, it is unrealistic to assume that mortality variation is the same at all ages.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-10/mortality-pensioner-mortality-variation/","title":"Mortality: Pensioner mortality variation","text":"<p>Modelling the mortality of DB pensioners is, in many ways, about as easy as mortality modelling gets \u2013 experience data is typically very high quality and often strongly credible, anti-selection is not usually a big deal and the available rating factors are limited (which constrains model complexity).</p> <p>I suggest that it\u2019s even simpler in that, for most pension plan mortality modelling, it is sufficient to assume that pensioner mortality varies monotonically in one-dimension along a low-high mortality axis.</p> <p>Don\u2019t just take my word for it; here\u2019s how log mortality varies for the CMI\u2019s S4 male pensioner base tables:</p> <p> </p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol> <p>Just to be clear, while the convergence of mortality rates at the \u2018younger\u2019 ages (below, say, age 85) in the above graph is present in the SAPS data, the convergence at higher ages (where there are fewer data and data errors cause more distortion) is actually enforced by the CMI, i.e. not 100% driven by the SAPS data. This is still evidence, albeit of a different type, in that</p> <ul> <li>the CMI has invested considerable resources in understanding high age mortality \u2013 see the HAMWP reports WP100 and WP122, and</li> <li>there is a huge amount of diverse mortality expertise amongst the CMI volunteers and secretariat, with the SAPS committee\u2019s approach itself having evolved in line with improved understanding<sup>1</sup>.</li> </ul>"},{"location":"2025-10/mortality-pensioner-mortality-variation/#general-model","title":"General model","text":"<p>What this means is that we can represent most of the information present in the above S4 base tables on a one dimensional scale, i.e.</p> <pre>\\[\\mu_x(\\beta)= \\mu_x^\\text{ref} \\exp\\big(\\beta\\,\\psi_x\\big)\\]</pre> <p>where \\(x\\) is age, \\(\\mu_x^\\text{ref}\\) is a common base mortality and \\(\\psi_x\\) is a common pattern of mortality variation by age that looks something like this:</p> <p> </p> <p>Identifying variation by individual and by time</p> <p>The age pattern of contemporary variation in mortality by individual is remarkably similar to the age pattern of mortality variation over the medium-to-longer timescales; so much so that it\u2019s tempting to identify them, i.e. assume they\u2019re the same thing.</p> <p>(This is not true for pensioner mortality variation over short timescales \u2013 it typically increases with age.)</p> <p>Variation (\\(\\psi_x\\)) tending to zero at high ages is equivalent to mortality converging at high ages.</p> <p>I\u2019ve used CMI S4 because it\u2019s the gold standard for UK DB pensioner base mortality, but this pattern of a common convergence pattern by age is generally applicable for DB pensioners. </p> <p>Slightly more strongly, I assert that the complexity of modelling mortality variation for DB pensioners can reasonably be reduced to modelling the variation in a single parameter for each of male pensioners, female retirees and female dependants.</p> <p>And because \\(\\psi_x\\gt0\\), this variation is monotonic.</p> <p>Insight 14. The bulk of pension plan mortality variation can be captured on a monotonic one dimensional scale</p> <p>Modelling base mortality for UK DB pension plans can reasonably be reduced to modelling a single parameter for each of male pensioners, female retirees and female dependants, i.e.</p> <pre>\\[\\mu_{it}(\\beta)= \\mu_{it}^\\text{ref} \\exp\\big(\\beta\\psi_x\\big)\\]</pre> <p>where \\(x\\) is age as a function of birth date from individual data \\(i\\) and time \\(t\\), \\(\\mu_{it}^\\text{ref}\\) is a common base mortality and \\(\\psi_x\\) is a common (non-negative) pattern of mortality variation by age that tends to zero at high ages.</p> <p>[All mortality insights]</p>"},{"location":"2025-10/mortality-pensioner-mortality-variation/#related-points","title":"Related points","text":"<p>There is a lot more to say in relation to the shape of mortality and the shape of mortality variation, and I hope to come back to this in a future article. For now, I\u2019ll restrict myself to a few passing comments:</p> <ol> <li> <p>I expect that some models in current use do not make the same simplifying assumption as above and that their authors believe that the added complexity makes their models superior. While not dismissing this out of hand, I would make a couple of points:</p> <ul> <li> <p>I suggest that there is more information content in allocating individuals to lower or higher mortality than there is in the precise shape of mortality by age.</p> </li> <li> <p>I\u2019ve had opportunities over the years to compare the postcode-based mortality model Miles Blackford and I developed at Aon with a couple of others produced by competitors I respected, and my empirical observation (based on an admittedly small sample) is that they came up with broadly similar results<sup>3</sup>.</p> </li> </ul> </li> <li> <p>Using a common pattern of variation between sub-groups is potentially more robust than calibrating sub-groups separately. For instance, I doubt that some of the shape complexity in the S4 variation graph at the top of this article is genuinely predictive<sup>5</sup>.</p> </li> <li> <p>Incorporating ill health mortality would require an additional variation component, i.e. variation would no longer be one-dimensional<sup>6</sup>. But allowing for ill-health mortality in pension plans (a)\u00a0does not usually move the needle much in terms of overall PV impact and (b)\u00a0comes with complications that can cause models to be less robust in practice<sup>7</sup>.</p> </li> <li> <p>It is reasonably well known that the apparent plateauing of observed population mortality at high ages does not necessarily mean that underlying individual mortality plateaus at high ages<sup>8</sup>. A similar caveat applies to apparent mortality convergence. But this is notoriously hard to determine one way or another and, in my experience, is often relegated to the too-hard category (if it\u2019s even thought about at all).</p> </li> </ol> <p>Next article: Good things come to those who weight (I)</p> <p>So far, the articles in this series have been mostly uncontentious. While treating \\(\\text{A}\\) and \\(\\text{E}\\) measures as foundational is, I think, intuitive, expressive and concise (and something that I have not seen elsewhere), it is ultimately just a nice way of expressing reasonably standard maths.</p> <p>I have taken care to ensure that the option of using a weight has been preserved in the formalism because this matters but it is often overlooked. Indeed, it is a feature of this formalism that a weight is required<sup>9</sup> (even if it\u2019s simply \\(1\\)).</p> <p>The next article will be the first of three on using weighted statistics to assess pension liabilities. Now this is contentious in some circles, to the point of being attacked as invalid. An alternative view is that it marks a dividing line between actuarial work and abstract statistics. You be the judge.</p> <ol> <li> <p>For instance, variation by age in log-mortality for the S1 series initially converges by age but then actually starts to diverge before being forced to reconverge again at the very highest ages. And for the S2 series (which predated the HAMWP), convergence was enforced at age\u00a095, which I now think is too early<sup>2</sup>.\u00a0\u21a9</p> </li> <li> <p>I was on SAPS committee when it produced S2, and so I\u2019m partly responsible for that too-early age convergence. On the plus side, repeated concerns about getting high age mortality SAPS mortality right was one of the motivations behind setting up the HAMWP.\u00a0\u21a9</p> </li> <li> <p>This does not mean that all postcode-based or other base mortality models are good; far from it. In my time I\u2019ve encountered</p> <ul> <li>a model that produced results that defied actuarial common sense, which I suspect was down to users misunderstanding the inputs required and / or maybe software error,</li> <li>models constructed from promising-sounding components but where the resulting models were not themselves tested statistically rigorously<sup>4</sup> against actual data, and</li> <li>a mortality modelling process that, by its very design, would produce biased results.</li> </ul> <p>\u21a9</p> </li> <li> <p>By \u2018rigorously\u2019, I mean testing the resulting model using leave-one out cross validation on mortality experience data by pension plan, or some variation thereof.\u00a0\u21a9</p> </li> <li> <p>This is not a slight on the CMI \u2013 the tables produced by CMI\u2019s SAPS committee are exemplars for actuarial professions in other locales.\u00a0\u21a9</p> </li> <li> <p>The same is true of any select mortality.\u00a0\u21a9</p> </li> <li> <p>Problems with incorporating ill health mortality in pension plan modelling include:</p> <ul> <li>Ill-health pensioner experience data is typically not very credible (because ill-health pensioners are a small fraction of the overall pensioner population and because the excess mortality from ill-health declines with time).</li> <li>Many UK pension plans have tightened their criteria for ill health retirement over time, which ill health status is an incoherent rating factor and using it will bias forecasts.</li> <li>Pension plan ill health criteria are not comparable between pension plans.</li> <li>Ill health retirement cannot be used as a rating factor for individuals yet to retire without further assumptions, which are themselves hard to calibrate.</li> </ul> <p>\u21a9</p> </li> <li> <p>This is an example of a potential ecological fallacy.\u00a0\u21a9</p> </li> <li> <p>As the function argument to the \\(\\text{A}\\) and \\(\\text{E}\\) measures.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-i/","title":"Mortality: Good things come to those who weight (I)","text":"<p>There was a time when actuaries understood that amounts-weighted statistics are the most appropriate ones for assessing DB pension plan liabilities. After all, </p> <ul> <li>the objective is to value liabilities,</li> <li>liability is proportional to pension amount, and</li> <li>pension amount is a strong predictor of mortality,</li> </ul> <p>and so weighting the experience data by what actually matters seems sensible.</p> Distribution of individuals (\u2018lives-weighted\u2019) Distribution of pension amount (\u2018amounts-weighted\u2019) <p>But then some actuaries noticed that statistics textbooks do not mention weighting by amounts, and so they decided that it must be wrong.</p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol> <p>At least I think this must have happened on occasion, because I\u2019ve seen some horrors in my time where actuaries discarded simple amounts-weighted A/E in favour of lives-weighted models but, in doing so, made their modelling worse, e.g. by introducing systematic bias.</p> <p>The is the first of three articles explaining why using weighted statistics makes sense in the context of DB pension plan mortality analysis. In this article, I\u2019ll define the objective, take stock of what we already know and introduce relevance. In the second article, I\u2019ll set out the maths and in the third consider the practical applications.</p>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-i/#1-whats-the-objective","title":"1. What\u2019s the objective?","text":"<p>The objective is to determine the best estimate and uncertainty of the present value<sup>1</sup> of liabilities. Along the way, yes, we will need to generate and test mortality models, but we do this because it enables us to meet the objective, not as an end in itself.</p> <p>Let me put this another way. If</p> <ul> <li>\\(L\\) is log-likelihood,</li> <li>\\(V\\) is present value of liabilities, and</li> <li>\\(\\beta\\) is a vector of the model parameters</li> </ul> <p>then the mechanism that ultimately determines how much \\(\\beta\\) matters is \\(\\partial V/\\partial \\beta\\), not \\(\\partial L/\\partial \\beta\\)<sup>2</sup>.</p> <p>At its most extreme, if you have a parameter \\(\\beta\\) for which \\(\\partial V/\\partial \\beta=0\\) then you don\u2019t care how well \\(\\beta\\) fits the data.</p>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-i/#2-what-do-we-already-know","title":"2. What do we already know?","text":"<p>If your domain is one in which very little is actually known then exploratory modelling will likely be a large part of the task of estimating the liabilities.</p> <p>But, when it comes to assessing mortality for a specific DB pension plan, a lot has already been determined:</p> <ul> <li>Historical mortality improvements are usually given, so we don\u2019t need to fit those<sup>3</sup>.</li> <li>We have a good a priori understanding of the likely range of base mortality \u2013 we may even have a literal prior mortality model (typically using postcodes etc).</li> <li>We know that pensioner mortality varies relatively smoothly by socio-economic type and that variation can be captured on a monotonic one dimensional scale (see previous article).</li> </ul> <p>What this means is that modelling base mortality for DB pensioners is not so much starting from scratch but more a matter of fine-tuning models we already have to hand.</p>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-i/#3-defining-and-incorporating-data-relevance","title":"3. Defining and incorporating data relevance","text":"<p>Actuaries make decisions on the relevance of mortality experience data all the time.</p> <ul> <li>It is routine to exclude older data over concerns that the population composition may have changed, that historical values of covariates cannot reliably be compared with current values or simply that mortality is time-heterogeneous.</li> <li>The CMI excludes data over 41 years old from its projections model<sup>4</sup>, and, more recently, it has under-weighted data for years materially affected by COVID-19.</li> <li>The mortality experience for pensioners below age 65 in DB pension plans is usually excluded because it is distorted by the heavier mortality of ill health early retirees.</li> <li>High age (e.g. over age 95) mortality experience is routinely excluded because of concerns over rating factor validity and because models can be sensitive to unnotified deaths at high ages<sup>5</sup>.</li> </ul> <p>What\u2019s worthy of note is how very fuzzy, uncertain and judgement-based the include / exclude decision is in contrast to the maths underlying most mortality modelling, which is sharp-edged, treating data as either 100% relevant or 100% irrelevant<sup>6</sup>. </p> <p>What happens if instead we try to incorporate relevance into the modelling itself? The fuzziness is not going to go away; all we\u2019re doing is moving fuzziness already present in the real world to inside our modelling. But this approach may provide us with insights, so let\u2019s give it a go.</p> <p>Relevance properties</p> <p>To ensure sensible behaviour, a relevance \\(r_a^b \\in [0,1]\\) should be </p> <ul> <li>100% self-relevant: \\(r_a^a=1\\)</li> <li>symmetric: \\(r_a^b= r_b^a\\), and</li> <li>consistent: \\(r_a^c \\ge r_a^b \\cdot r_b^c\\),</li> </ul> <p>where \\(a=it\\), \\(b=ju\\) and \\(c=kv\\).</p> <p>Or, equivalently, \\(-\\log r_a^b\\) should be a metric.</p> <p>First, we need to define it: a relevance<sup>7</sup> \\(r_{it}^{jt_0} \\in [0,1]\\) is the multiplicative factor to apply to the log-likelihood of the experience data for individual \\(i\\) at time \\(t\\) to assess liabilities for individual \\(j\\) as at the valuation date \\(t_0\\).</p> <p>I don\u2019t think this is contentious \u2013 relevance is not uncommon in modern statistical sampling. And I suspect that you didn\u2019t bat an eyelid when I mentioned above that the CMI under-weighted COVID-19 affected data in its Mortality Projections Model, and yet those weights are nothing more than relevances.</p> <p>We\u2019re interested in the relevance of mortality experience data when assessing pension plan liabilities. An obvious example, already alluded to, is that older data is typically less relevant than more recent data. A relevance-based framework can certainly handle that, but the specific type of relevance that I want to consider is the relevance of one individual\u2019s mortality experience to the valuation of a liability relating to another individual.</p> <p>To make progress we\u2019ll need to see some maths, which I\u2019ll set out in my next article.</p>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-i/#a-litmus-test","title":"A litmus test","text":"<p>In the meantime, I\u2019ll pose a simple question: if you were forced to choose between A/E on a lives or amounts-weighted basis as the sole available metric for assessing the liabilities of a typical UK DB pension plan which would you choose?</p> <p>Scenario</p> <p>To make this more concrete, imagine a portfolio comprising two groups as follows:</p> Group Lives Total liability Average liability Proportion by lives Proportion by liability A 1,500 \u00a3100m \u00a367k 75% 25% B 500 \u00a3300m \u00a3600k 25% 75% <p>Would you value the liabilities  for group\u00a0B, i.e. 75% of the liabilities, using unweighted data, i.e. 75% based on group\u00a0A?</p> <p>Neither is ideal, but only one of these options gets you a shot at achieving the objective, i.e. valuing the liabilities. The other may be a right answer but to a different question.</p> <p>Rico Mariani\u2019s pit of success</p> <p>Rico Mariani\u2019s 2003 ad lib expressed the design principle that users should end up with a good answer by default. The context was the design of the then new .NET coding framework and its associated C# language. The principle was reiterated by, amongst others, Jeff Atwood and Eric Lippert, which are well worth a read, even if you\u2019re not a coder.</p> <p>I suggest that it\u2019s a general design principle. To quote Mariani:</p> <p>Success has to be like falling into a big pit. You can\u2019t help but win. You try to do it wrong, but nope, you fell in The Pit of Success. You win again!</p> <p> If you\u2019re thinking that being forced to use a single metric is unrealistic then think again. Standardised processes that have to handle 50, 100 or more pension plan mortality analyses per year robustly and efficiently often collapse pension plan mortality experience down to one variable when mixing a pension plan\u2019s own mortality experience with the mortality predicted by a prior model. </p> <p>And, beyond that, there\u2019s an important principle at stake here: a minimum target for any modelling framework is that it should provide a sensible answer by default. In a very different context, Rico Mariani named this principle the \u2018pit of success\u2019 \u2013 see box out.</p> <p>I suggest that the equivalent in the context of DB pension plan mortality modelling is that a well-designed framework applied to a single parameter model should default to giving a reasonable estimate of mortality for valuing liabilities.</p> <p>For avoidance of doubt, I am not claiming that this excludes either the adoption of more complex models (commensurate with the amount of experience data) or a role for expert judgement \u2013 both are essential parts of the mortality modelling toolkit. But if the framework you\u2019re using does not default to giving a reasonable estimate of mortality for valuing liabilities in the simplest scenario then I think you have some questions to answer.</p> <p>Next article: Good things come to those who weight (II)</p> <p>In Part\u00a0II I\u2019ll set out some maths to make the above more concrete.</p> <p>And then in Part\u00a0III, the final article in this series, I\u2019ll address its practical application.</p> <ol> <li> <p>In the real world, we\u2019d probably also want projected cashflows, but let\u2019s keep things simple.\u00a0\u21a9</p> </li> <li> <p>Of course, \\(\\partial L/\\partial \\beta\\) (and \\(\\partial L^2/\\partial \\beta^2\\)) may lie on the pathway to determining \\(\\partial V/\\partial \\beta\\).\u00a0\u21a9</p> </li> <li> <p>Linking historical mortality improvements to projected future improvements requires experience data for much longer time periods and wider age ranges than are available for even the very largest pension plans, so this analysis is usually independent. Yes, there is often an attempt to map the socio-economic profile of an individual plan to a mortality projection, but this mapping is itself also usually determined a priori.\u00a0\u21a9</p> </li> <li> <p>This results in the questionable effect that if an atypical calendar year falls off the start of the 41 year data period then this dropping of shortly-to-be-out-of-date data can impact the CMI Model\u2019s current predictions.\u00a0\u21a9</p> </li> <li> <p>Survival is unlikely at very high ages, which means an unnotified death look statistically significant. Models typically also assume mortality convergence at very high ages, which amplifies the effect further still.\u00a0\u21a9</p> </li> <li> <p>For the avoidance of doubt, I am not suggesting that using a well-defined framework to analyse data and create models is unreasonable, not least because it helps define and corral the fuzziness. But if (a)\u00a0it\u2019s not clear which data to include or exclude and (b)\u00a0inclusion / exclusion has a material impact on the results, then this in itself implies that something is missing from the modelling.\u00a0\u21a9</p> </li> <li> <p>Relevance is also known as importance or reliability. Beware that these words are also used to described other types of weights, so it\u2019s important to check definitions. And sometimes weighting log-likelihood is assumed to be such an obvious concept that it is not given a special name at all.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-ii/","title":"Mortality: Good things come to those who weight (II)","text":"<p>Maths!</p> <p>This article is almost all maths.</p> <p>If that\u2019s not your thing then I suggest that you skip ahead to Part\u00a0III.</p> <p>In the previous article, I noted that the the ultimate objective when assessing base pensioner mortality for DB plans is to determine the present value of the liabilities, as opposed to selecting and calibrating an abstract model.</p> <p>In this article, I\u2019ll use a simple framework to express this mathematically.</p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-ii/#starting-point","title":"Starting point","text":"<p>These are our givens:</p> <ol> <li> <p>The objective is to determine the best estimate and uncertainty of the present value of liabilities.</p> </li> <li> <p>Our mortality model has a single scalar parameter \u2013 see e.g. this previous article in relation to DB pensioner mortality.</p> </li> <li> <p>We are provided with a measure of relevance \u2013 see the definition here. This quantifies the real life fuzziness around which data to rely on when estimating mortality.</p> </li> </ol> <p>To ease the notational burden, I\u2019ll make the following simplifications:</p> <ul> <li> <p>Instead of indexing everywhere by individual \\(i\\) and time \\(t\\), use a combined index, e.g. \\(a\\). (For individuals in the valuation dataset, \\(t\\) is always the valuation \u2018as at date\u2019 and so these are already effectively indexed by individual.)</p> </li> <li> <p>Instead of writing out integrals over time and sums over individuals for experience data, denote this using the discrete summation symbol, e.g. \\(\\sum_a \\ell_a\\)<sup>1</sup>.</p> </li> <li> <p>Instead of writing log-likelihood or liability value as a function of a mortality, e.g. \\(\\ell(\\mu(\\beta))\\) or \\(v(\\mu(\\beta))\\), write them as direct functions of the mortality model parameters, e.g. \\(\\ell(\\beta)\\) and \\(v(\\beta)\\).</p> </li> </ul> <p>Some definitions:</p> <ul> <li>We\u2019ll initially assume our mortality model takes \\(n\\) parameters, represented as a vector, e.g. \\(\\beta\\). (We\u2019ll specialise to the \\(n=1\\) case later.)</li> <li>\\(\\ell_a(\\beta)\\) is the log-likelihood of the E2R for \\(a\\in\\text{Exp}\\) using our mortality model, where \\(\\text{Exp}\\) is the experience data.<sup>1</sup></li> <li>\\(v_b(\\beta)\\) is the present value of liabilities for valuation individual \\(b\\in\\text{Val}\\) using our mortality model, where \\(\\text{Val}\\) is the valuation data.</li> <li>\\(r_a^b\\in[0,1]\\) is the relevance of \\(a\\) to \\(b\\), with \\(r_a^a=1\\) and \\(r_a^b=r_b^a\\).</li> <li>\\('\\) and \\(''\\) are the vector first derivative and matrix second derivative with respect to \\(\\beta\\).</li> <li>\\(^\\text{T}\\) is vector or matrix transpose.</li> </ul> <p>We\u2019ll also impose the simplest non-trivial conditions to make things tractable: </p> <ul> <li> <p>\\(\\ell_a(\\beta)\\) is approximately quadratic, with \\(\\ell''_a\\) negative definite, and</p> </li> <li> <p>\\(v_b(\\beta)\\) is approximately linear in \\(\\beta\\), with \\(v'_b\\) positive<sup>2</sup>.</p> </li> </ul> <p>This graphic illustrates</p> <ul> <li> <p>indexing the experience data using a (nominally discrete) single-dimensional index \\(a\\) in place of the two dimensions of individual \\(i\\) and (continuous) time \\(t\\) (within each individual\u2019s E2R), and</p> </li> <li> <p>how elements of the experience data are related by relevance to elements of the valuation data.</p> </li> </ul> <p> </p>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-ii/#roadmap","title":"Roadmap","text":"<p>To give you an idea of where we\u2019re headed, here is a roadmap:</p> <ul> <li> <p>First, we\u2019ll define best estimate liability taking account of relevance.</p> </li> <li> <p>Then we\u2019ll look for an equivalent overall parameter that gives the same answer.</p> </li> <li> <p>To complete the circle, we\u2019ll re-express the derivation of that equivalent overall parameter in terms of weighted log-likelihood.</p> </li> <li> <p>Finally, we\u2019ll address parameter uncertainty.</p> </li> </ul>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-ii/#best-estimate-liability","title":"Best estimate liability","text":"<p>The relevant log-likelihood for each individual to be valued \\(b\\in\\text{Val}\\) is</p> <pre>\\[L_b(\\beta)=\\sum_{a\\in\\text{Exp}}r_b^a\\ell_a(\\beta)\\tag{17}\\]</pre> <p>This defines an estimate of the model parameter for \\(b\\in\\text{Val}\\) as</p> <pre>\\[\\hat\\beta_b = \\underset{\\beta}{\\arg\\max}\\, L_b(\\beta)\\]</pre> <p>or, equivalently, given that the \\(\\ell_a\\) are approximately quadratic, we can solve the differential equation</p> <pre>\\[L'_b(\\hat\\beta_b)=0\\tag{18}\\]</pre> <p>The best estimate of total liability value is the sum of each individual\u2019s liabilities evaluated using the parameter fitted to the data relevant to that individual, i.e.</p> <pre>\\[\\hat V=\\sum_{b\\in\\text{Val}}v_b(\\hat\\beta_b)\\tag{19}\\]</pre>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-ii/#equivalent-overall-parameter","title":"Equivalent overall parameter","text":"<p>Equation \\((19)\\) is fine in theory but it\u2019d be a nightmare to implement because each individual in the valuation data requires their own mortality model to be calibrated and selected.</p> <p>What if, instead, we could derive a single overall parameter vector estimate, \\(\\hat\\beta\\), that gave the same total value, i.e.</p> <pre>\\[\\hat V=\\sum_{b\\in\\text{Val}}v_b(\\hat\\beta)\\]</pre> <p>The assumed approximate linearity of \\(v_b\\) means that this is equivalent to the condition</p> <pre>\\[\n0\n=\\sum_{b\\in\\text{Val}}\\left\\{v_b(\\hat\\beta)-v_b(\\hat\\beta_b)\\right\\}\n\\approx\\sum_{b\\in\\text{Val}} v'_b{}^\\text{T}  \\left\\{\\hat\\beta-\\hat\\beta_b\\right\\}\n\\tag{20}\n\\]</pre> <p> \\(v'_b\\) is written without an argument because, by assumption, it is approximately constant.</p> <p>This means that a single overall estimate \\(\\hat\\beta\\) that complies with equation \\((20)\\) will provide (approximately) the same overall value as the complex approach embodied in equation \\((19)\\).</p> <p>As it stands, equation \\((20)\\) still requires that we calculate parameters \\(\\hat\\beta_b\\) separately per individual. Is there a way to sidestep this?</p> <p>One approach is to try to sum up the log-likelihoods in such a way that the maximum of that sum is automatically at \\(\\hat\\beta\\). Specifically, we\u2019d like a scalar weight, \\(\\tilde w_b\\) for \\(b\\in\\text{Val}\\) such that equation \\((19)\\) holds if</p> <pre>\\[\\hat\\beta=\\underset{\\beta}{\\arg\\max} \\sum_{b\\in\\text{Val}} \\tilde w_b \\, L_b(\\hat\\beta)\\]</pre> <p>or, equivalently,</p> <pre>\\[\\sum_{b\\in\\text{Val}} \\tilde w_b \\, L'_b(\\hat\\beta)=0\\tag{20a}\\]</pre> <p>The assumption that \\(\\ell_a\\) is approximately quadratic means that so is \\(L_b\\) and hence</p> <pre>\\[L'_b(\\hat\\beta) \n\\approx L'_b(\\hat\\beta_b)+L''_b \\left\\{\\hat\\beta-\\hat\\beta_b\\right\\} \n= -I_b \\left\\{\\hat\\beta-\\hat\\beta_b\\right\\}\\tag{22}\\]</pre> <p>where I\u2019ve used equation \\((18)\\) and, for convenience, defined the relevant information (matrix) as <pre>\\[I_b = -L''_b = -\\sum_{a\\in\\text{Exp}}r_b^a\\ell''_a \\tag{23}\\]</pre> <p> \\(I_b\\), \\(L''_b\\) and \\(\\ell''_a\\) are written without an argument because they are approximately constant (similar to \\(v'_b\\), as noted above).</p> <p>This enables us to rewrite equation \\((21a)\\) approximately as</p> <pre>\\[\\sum_{b\\in\\text{Val}} \\tilde w_b I_b \\left\\{\\hat\\beta-\\hat\\beta_b\\right\\}=0\\]</pre> <p>and comparison with equation \\((20)\\) suggests<sup>2</sup><sup>3</sup><sup>4</sup></p> <pre>\\[\\tilde w_b=I_b^{-1} v'_b\\tag{24}\\]</pre> <p>The single constraint implied by (the scalar) equation \\((20)\\) does not define a scalar weight if there are \\(n&gt;1\\) degrees of freedom in (the vector) equation \\((21a)\\). This is manifest in equation \\((24)\\), which defines \\(\\tilde w_b\\) as a vector, not a scalar, and in which case equation \\((21a)\\) would need to be rewritten as</p> <pre>\\[\\sum_{b\\in\\text{Val}} \\tilde w_b^\\text{T} \\, L'_b(\\hat\\beta)=0\\tag{21b}\\]</pre>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-ii/#weighted-log-likelihood","title":"Weighted log-likelihood","text":"<p>While this approach is not fruitful in the \\(n&gt;1\\) case, one of our givens above is that our mortality model has a single scalar parameter, i.e. \\(n=1\\), which means that, in our scenario, equation \\((24)\\) does define a scalar weight.</p> <p>I used \\(\\tilde w_b\\) to denote the weighting of relevant log-likelihood indexed by valuation individual \\(b\\) because I was saving \\(w_a\\) to denote the equivalent weight indexed by experience datum, i.e.</p> <pre>\\[w_a =\\sum_{b\\in\\text{Val}} r_a^b\\tilde w_b\\]</pre> <p>Changing order of summation, we have</p> <pre>\\[\\sum_{b\\in\\text{Val}} \\tilde w_b L_b(\\beta)=\\sum_{b\\in\\text{Val}} \\tilde w_b \\sum_{a\\in\\text{Exp}} r_b^a\\ell_a(\\beta)= \\sum_{a\\in\\text{Exp}} w_a\\ell_a(\\beta)\\]</pre> <p>and hence the general weight to apply to the experience data is<sup>4</sup></p> <pre>\\[w_a=\\sum_{b\\in\\text{Val}} r_a^b \\, I_b^{-1} v'_b\\tag{25}\\]</pre> <p>Insight 15. Weighted log-likelihood automatically estimates liabilities correctly for single scalar parameter models when provided with relevance</p> <p>If (a)\u00a0a mortality model has a single scalar parameter and (b)\u00a0relevance is provided then maximising log-likelihood weighted by</p> <pre>\\[w_{it}=\\sum_{j\\in\\text{Val}} r_{it}^{jt_0} \\, I_j^{-1} v'_j\\]</pre> <p>automatically results in the best estimate of the present value of liabilities.</p> <p>In the above,</p> <ul> <li>\\(r_{it}^{jt_0}\\) is the relevance of the log-likelihood of the E2R of individual \\(i\\) at time \\(t\\) to individual \\(j\\) in the valuation data as at the valuation date \\(t_0\\),</li> <li>\\(I_j\\) is the relevant information matrix for valuation individual \\(j\\), and</li> <li>\\(v'_j\\) is derivative of liability value for valuation individual \\(j\\) with respect to the model parameter \\(\\beta\\).</li> </ul> <p>For further definitions, see article body.</p> <p>[All mortality insights]</p> <p>I suggest that the above means is that, in a DB pensioner context, if we already have a reasonable default mortality (so that we can calculate \\(I\\) and \\(v'\\) in advance) then we can define a weight such that maximising weighted log-likelihood results in the correct overall parameter for assessing liability value for a single parameter model.</p> <p>I\u2019ll expand on this in P\u00a0III.</p>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-ii/#parameter-uncertainty","title":"Parameter uncertainty","text":"<p>Best estimates are meaningless without some measure of uncertainty. I\u2019m not going to plow through yet more maths here and so instead I\u2019ll simply state results<sup>5</sup>.</p> <p>We can estimate the variance in best estimate liabilities as </p> <pre>\\[\n\\text{Var}(\\hat V)\n\\approx \\sum_{b,c\\in\\text{Val}}v'_b{}^\\text{T}\\,\\text{Cov}(\\hat\\beta_b,\\hat\\beta_c)\\,v'_c\n\\approx-\\!\\sum_{a\\in\\text{Exp}}u_a^\\text{T}\\,\\ell''_a\\,u_a\n\\]</pre> <p>where \\(u_a\\) is the vector</p> <pre>\\[u_a=\\sum_{b\\in\\text{Val}} \\sqrt{r_a^b}\\,I_b^{-1}v'_b\\tag{26}\\]</pre> <p> Contrast the uncertainty weight in equation \\((26)\\) with the weight used to derive the best estimate in equation \\((25)\\) above.</p> <p>Given our assumptions, we have</p> <pre>\\[\\text{Var}(\\hat V)\\approx V'{}^\\text{T}\\text{Var}(\\hat \\beta)V'\\]</pre> <p>where</p> <pre>\\[V' = \\sum_{b\\in\\text{Val}}v'_b = -\\!\\sum_{a\\in\\text{Exp}}\\ell''_a w_a \\]</pre> <p>In the \\(n=1\\) case, this defines an estimate of the variance of \\(\\hat\\beta\\) as</p> <pre>\\[\\text{Var}(\\hat\\beta) \\approx (V')^{-2}\\;\\text{Var}(\\hat V)\\tag{27}\\]</pre> <p>Insight 16. A different weight is required to determine uncertainty in the presence of relevance</p> <p>The log-likelihood weight to determine uncertainty that corresponds to the best estimate weight in Insight\u00a015 is</p> <pre>\\[u_{it}=\\sum_{j\\in\\text{Val}} \\sqrt{r_{it}^{jt_0}} \\, I_j^{-1} v'_j\\]</pre> <p>[All mortality insights]</p>"},{"location":"2025-10/mortality-good-things-come-to-those-who-weight-ii/#summary","title":"Summary","text":"<p>Let\u2019s take stock: if our underlying mortality model has a single scalar parameter, as is typically the case for mortality models for DB pensioners, then </p> <ol> <li> <p>maximising log-likelihood weighted by \\(w\\) leads to the best estimate of liability value, and</p> </li> <li> <p>the second differential of log-likelihood weighted by \\(u^2\\) estimates parameter (and liability) uncertainty.</p> </li> </ol> <p>Next article: Good things come to those who weight (III)</p> <p>There remain questions of practicality and generalisability.</p> <p>I\u2019ll conclude this series by addressing those questions in Part\u00a0III](/2025-11/mortality-good-things-come-to-those-who-weight-iii/).</p> <ol> <li> <p>Sums over infinitesimals using measures are usually written with \\(\\int\\), not \\(\\sum\\). But I think using a single \\(\\sum\\) symbol makes it easier to see what\u2019s going on compared with having to write sums over individuals and integrals over time everywhere. If it troubles you, imagine there\u2019s a time grid with spacing \\(\\delta t\\), replace \\(\\ell_a\\) with \\(\\delta\\ell_{it}\\) and take the limit \\(\\delta t\\rightarrow 0\\).\u00a0\u21a9\u21a9</p> </li> <li> <p>Althought I\u2019ve assumed \\(v'_b\\) is positive, the sign of \\(v'_b\\) doesn\u2019t matter provided it\u2019s consistent: if \\(v'_b\\lt0\\) then replace \\(v'_b\\) everywhere with \\(-v'_b\\). Indeed, because mortality models are usually arranged so that higher \\(\\beta\\) increases mortality and the liability in a pensions context is an annuity, I\u2019d expect \\(v'_b\\) to be negative.\u00a0\u21a9\u21a9</p> </li> <li> <p>\\(I_b\\) is known to be invertible because it is positive definite, which in turn is because, by assumption, the \\(\\ell''_a\\) are negative definite.\u00a0\u21a9</p> </li> <li> <p>Pointed out to me by Andy Harding \u2013 thank you.\u00a0\u21a9\u21a9</p> </li> <li> <p>These results are unchecked, so please review before you apply them.\u00a0\u21a9</p> </li> </ol>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/","title":"Mortality: Good things come to those who weight (III)","text":"<p>This is the final article of three on weighted mortality analysis.</p> <p>If you haven\u2019t already read Part\u00a0I then I recommend you do that first.</p> <p>And if you\u2019re really keen to understand the maths then give Part\u00a0II a go, but it\u2019s not essential.</p> <p>In this article, I\u2019ll review the implications and provide some practical recommendations.</p> <p>Articles in this series</p> <ol> <li>Measures matter</li> <li>A over E</li> <li>Log-likelihood</li> <li>Proportional hazards</li> <li>Suddenly AIC</li> <li>Overdispersion and quasi-log-likelihood</li> <li>Incoherent rating factors</li> <li>Pensioner mortality variation</li> <li>Good things come to those who weight: Part\u00a0I, Part\u00a0II and Part\u00a0III</li> </ol>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#relevance-revisited","title":"Relevance revisited","text":"<p>But first, let\u2019s revisit relevance.</p> <p>Statistics text books typically assume not only that experience data is either 100% relevant or 100% irrelevant, but that determining whether data is relevant or irrelevant is obvious. And in many cases this may be reasonable \u2013 if we are testing the reliability<sup>1</sup> of widgets then get some widgets and test them, which sounds well-defined<sup>2</sup>.</p> <p>But what experience data is relevant for estimating the mortality for a current or future pensioner in the XYZ Pension Plan? There are two dimensions, time and individual (which are non-exclusive, you can do one or both).</p>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#time-based-relevance","title":"Time-based relevance","text":"<p>The notion that relevance typically decreases as time elapses is uncontentious, at least conceptually:</p> <ul> <li> <p>This doesn\u2019t imply that the most recent data is always the most relevant. For instance, we may also decide to treat COVID-impacted years as less relevant.</p> </li> <li> <p>Smoothly varying relevance by time is not usually a built in feature. Instead it is usually left as an ad hoc practitioner decision, and typically implemented on an all-or-nothing basis.</p> </li> </ul> <p>An example of a time-based relevance<sup>3</sup> is</p> <pre>\\[r_s^t=\\exp\\!\\big(\\!-\\phi\\,\\big|t-s\\big|\\big)\\]</pre> <p>where \\(s\\) and \\(t\\) are dates (measured in years) and \\(\\phi&gt;0\\).</p> <p>It is a special case if relevance is purely time-based, i.e. no variation by individual, because we can shortcut all the above simply by scaling the data by relevance and proceeding with the lives-weighted approach. While, yes, this is a relevance-based approach, the maths is so trivial that it is usually treated as obvious and goes unremarked<sup>4</sup>.</p> <p>This is applicable beyond base mortality \u2013 I\u2019d recommend using time-based relevance for calibration of all mortality projection and stochastic mortality models (including the CMI Mortality Projections Model).</p> <p>Insight 17. Always allow for time-based relevance</p> <p>Allowing for time-based relevance, e.g. using </p> <pre>\\[r_s^t=\\exp\\!\\big(\\!-\\phi\\,\\big|t-s\\big|\\big)\\]</pre> <p>where \\(s\\) and \\(t\\) are dates (measured in years) and \\(\\phi&gt;0\\), is to be preferred in all mortality modelling contexts because</p> <ul> <li>it automatically allows for the decay in relevance as time elapses, and</li> <li>compared with fixed windows, leaves models less sensitive to the falling away of more historical data.</li> </ul> <p>If relevance is purely time-based then this can be accomplished simply by scaling the experience data.</p> <p>[All mortality insights]</p>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#individual-based-relevance","title":"Individual-based relevance","text":"<p>The more interesting question is relevance by individual.</p> <p>I suggest that</p> <ul> <li> <p>it\u2019s unreasonable to assume that the mortality experience of all individuals in the same pension plan is 100% relevant<sup>5</sup>, and</p> </li> <li> <p>relevance should take account of pension given that it\u2019s the one simple fact that is strongly correlated with both (a)\u00a0mortality and (b)\u00a0liability impact.</p> </li> </ul> <p>If you have a lot of experience data, then you will likely end up sub-dividing your model by pension band (or equivalent), in which this probably<sup>6</sup> doesn\u2019t matter.</p> <p>But if you don\u2019t have a lot of data (or if you calibrate any covariates across all lives) then not taking account of relevance will likely matter. A particularly fertile area is processes that handle lots of cases each of which has relatively small amounts of experience data.</p> <p>Given that the underlying variables feeding into data relevance need to be generally available, simple and robust, an obvious<sup>7</sup> candidate for individual relevance is log-pension, for example<sup>3</sup></p> <pre>\\[\\begin{aligned}\nr_p^q\n&amp;=\\exp\\!\\big(\\!-\\psi\\,\\big|\\log q-\\log p\\big|\\big)\n\\\\[1em]\n&amp;= \\min\\!\\big(p/q,q/p\\big)^\\psi\n\\end{aligned}\\]</pre> <p>where \\(p\\) and \\(q\\) are pension amounts (revalued to the same date for consistency) and \\(\\psi&gt;0\\).</p>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#behaviour-in-a-simple-case","title":"Behaviour in a simple case","text":"<p>In order to help us intuit what\u2019s going on, we can ask what happens if relevance is always 0 or 1? In this case, relevance partitions<sup>8</sup> the experience data and valuation data into a finite number of groups labelled \\(G\\)<sup>9</sup>, within each of which all experience is 100% relevant to all valuation individuals. </p> <p>The relevance approach collapses to<sup>10</sup> </p> <pre>\\[\\begin{aligned}\n\\hat\\beta &amp;\\approx \\big(V'\\big)^{-1}\\sum_G V'_G \\, \\hat\\beta_G\n\\\\[1em]\n\\text{Var}\\big(\\hat\\beta\\big) &amp;\\approx \\big(V'\\big)^{-2}\\sum_G \\big(V'_G\\big)^2 I_G^{-1}\n\\end{aligned}\\]</pre> <p>where</p> <ul> <li>\\(V_G\\) is the sum of liability values for individuals in group \\(G\\), with the total being \\(V=\\sum_G V_G\\),</li> <li>\\(L_G\\) is the sum of log-likelihood for the E2Rs in group \\(G\\), with the relevant information for group \\(G\\) being \\(I_G=-L''_G\\), and</li> <li>\\('\\) and \\(''\\) are first and second derivatives with respect to the mortality model parameter \\(\\beta\\).</li> </ul> <p>In other words, </p> <ul> <li> <p>the overall best estimate is the sum of the per-group best estimates weighted by liability impact (\\(V'_G\\)), and</p> </li> <li> <p>the variance of the overall best estimate reflects both (a)\u00a0liability impact (\\(V'_G\\)) and (b)\u00a0the amount of experience data available (\\(I_G\\)) per group.</p> </li> </ul> <p>This is self-evidently the best estimate for liability value and its uncertainty given the assumptions and so it is reassuring that the general approach gives the correct answer in this case.</p>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#application","title":"Application","text":"<p>In Part\u00a0II, we showed that if</p> <ul> <li>a mortality model has a single scalar parameter, and</li> <li>we are provided with a relevance of the log-likelihood of the E2R for individual \\(i\\) at time \\(t\\) to individual \\(j\\) in the valuation data as at the valuation date \\(t_0\\),</li> </ul> <p>It\u2019s not amounts-weighted A/E vs lives-weighted modelling</p> <p>One reason the \u2018lives-weighted is the only valid approach\u2019 view has propagated is because some actuaries think the only combinations of options are</p> <ul> <li>amounts-weighted A/E, or</li> <li>sophisticated lives-weighted modelling,</li> </ul> <p>when actually the choices of weight and modelling approach are orthogonal.</p> <p>then we can define a weight such that maximising the weighted log-likelihood automatically results in the best estimate of the present value of liabilities when using that single parameter mortality model.</p> <p>I suggest that it is best practice to use an approach that</p> <ul> <li> <p>takes explicit account of liability impact (given that assessing liabilities is our objective), and</p> </li> <li> <p>in particular, defaults to unbiased liability valuation results regardless of the quantum of experience data available (which is not automatically the case for lives-weighted statistics).</p> </li> </ul> <p>Using a weighted log-likelihood as derived to fit DB pensioner base mortality models meets these criteria and \u2013 as demonstrated by the previous articles in this series \u2013 leaves standard modelling machinery unchanged.</p> <p>Insight 18. Use relevance for calibrating and selecting DB pensioner base mortality models</p> <p>Using the weights by \\(w\\) and \\(u\\) as defined in Insights\u00a015 and 16 respectively to calibrate and select DB pensioner base mortality models </p> <ul> <li> <p>takes explicit account of liability impact, and</p> </li> <li> <p>defaults to sensible results regardless of the quantum of experience data available.</p> </li> </ul> <p>The following Insights need to be restated to accommodate relevance:</p> <ul> <li> <p>Insight\u00a05 (allowing for overdispersion \\(\\Omega\\)) becomes</p> <pre>\\[\\text{Var}\\big(\\text{A}w-\\text{E}w\\big)=\\Omega\\,\\mathbb{E}\\big(\\text{E}u^2\\big)\\]</pre> </li> <li> <p>The equations for Insights\u00a09 and Insights\u00a010 are unchanged as</p> <pre>\\[\\begin{aligned}\n\\text{Var}\\big(\\hat\\beta\\big)\\mathrel{\\hat=} \\Omega\\,\\mathbf{I}^{-1}\\mathbf{J}\\mathbf{I}^{-1}\n\\\\[1em]\nL_\\text{P}= L(\\hat\\beta)-\\text{tr}\\big(\\mathbf{J}\\mathbf{I}^{-1}\\big)\n\\end{aligned}\\]</pre> <p>But \\(\\mathbf{J}\\) is redefined as \\(\\text{E}u^2XX^\\text{T}\\), i.e. weighted by \\(u^2\\) rather than \\(w^2\\).</p> </li> </ul> <p>[All mortality insights]</p> <p>Being able to define a weight in this way depends on specific features of DB pensioner mortality and so it is not general. But there are questions to answer if actuaries do not consider for liability impact in their statistical analysis<sup>11</sup>.</p>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#alternatives","title":"Alternatives","text":"<p>The infinite data fallacy</p> <p>The infinite data fallacy is that there will always be sufficient experience data to determine features critical to your model<sup>12</sup>.</p> <p>I\u2019ve used the future tense because the assumption is usually made in the context of a hypothetical future case, as opposed to a common concrete dataset that we can all examine.</p> <p>The classic example is assuming that there will be sufficient experience data to determine dependence of mortality on pension amount. (One of the many reasons why \u2018Just include pension as a covariate\u2019 is a red light.)</p> <p>The point is that in the presence of infinite data we cannot distinguish between different methods, because they will all work (including weighting by \u2018anything I feel like\u2019).</p>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#1-just-include-pension-as-a-covariate","title":"1. \u2018Just include pension as a covariate\u2019","text":"<p>People reach for this throw away line frequently, but it\u2019s flawed.</p> <ul> <li> <p>It doesn\u2019t specify a model to be fitted or a concrete approach. Yes, sensible approaches to allow for pension as a covariate can be constructed but they have to work very hard to avoid bias in the absence of credible experience data.</p> </li> <li> <p>And that leads on to the second point, which I call the \u2018infinite data fallacy\u2019 \u2013 see box out. The throw away line implicitly assumes either (a)\u00a0there is sufficient experience data to capture the impact of pension as a covariate or (b)\u00a0if there isn\u2019t then it doesn\u2019t matter. But it most certainly does matter: the mortality (\u03bc) rates for the CMI\u2019s standard male pensioner base tables at age 75 are over 20%(!) higher for lives-weighted vs amounts-weighted. I\u2019d like an approach that works in all cases please.</p> </li> <li> <p>Allowing for \u2018pension as a covariate\u2019 doesn\u2019t help with other independent covariates \u2013 you\u2019ll still end up fitting these on a lives-weighted basis.</p> </li> <li> <p>Finally, people tend to gloss over the problems with using pension as a covariate.</p> </li> </ul>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#2-why-not-use-a-bayesian-approach-instead","title":"2. Why not use a Bayesian approach instead?","text":"<p>Bayesian approaches are prima facie attractive, and maybe they can be made to work in this context too.</p> <p>Whereas relevance is a standalone concept, the definition of a prior depends on the particular mortality model (because it\u2019s a prior for the parameters of a model), which I think makes using priors less flexible and less intuitive than relevance, and probably makes the maths more messy too.</p> <p>If you can get it to work then great (but note that if liability values don\u2019t figure in your approach then, whatever problem you think you\u2019ve solved, it isn\u2019t pricing or valuation related).</p>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#3-weight-by-pension-amount","title":"3. Weight by pension amount","text":"<p>In real life your systems may not be able to accommodate relevance, in which case I suggest just weight your analysis by pension amount<sup>13</sup>.</p> <p>People (including me) sometimes worry about abnormally large pensions distorting the results but this hasn\u2019t been a problem in practice:</p> <ul> <li>The variance will capture concentration.</li> <li>UK DB pensions have been capped by regulation for such a long time now that extremely large pensions no longer exist.</li> </ul> <p>It boils down to how you want to distribute your errors:</p> <p> </p> <p>If it\u2019s a choice between lives and amounts-weighted then, while both are \u2018wrong\u2019, the amounts-weighted is safer in a DB pensions context if your objective is to value liabilities.</p> <p>Insight 19. Prefer amounts-weighted to lives-weighted log-likelihood</p> <p>For DB pensioner mortality analysis, prefer statistics weighted by pension amount over lives-weighted.</p> <p>When there is a lot of experience data it won\u2019t matter; when there is only a little it will mitigate biased liability value estimates.</p> <p>[All mortality insights]</p>"},{"location":"2025-11/mortality-good-things-come-to-those-who-weight-iii/#conclusion","title":"Conclusion","text":"<p>If you believe</p> <ul> <li> <p>what matters is valuing liabilities correctly (as opposed to developing models that are good in some abstract sense),</p> </li> <li> <p>relevance of mortality experience varies by pension amount, and</p> </li> <li> <p>the variation in pension plan mortality can be parametrised primarily in one dimension as heavy vs light mortality,</p> </li> </ul> <p>then I think you should be using weighted log-likelihood when assessing base mortality for DB pensioners.</p> <p>End of this series</p> <p>This is the final article on my series on base mortality modelling in relation to DB pension plans. I hope you\u2019ve found some of it interesting or even useful.</p> <p>There will be more to come on mortality generally and other things too.</p> <ol> <li> <p>It is a linguistic convention that we use \u2018reliability\u2019 in an engineering context and \u2018mortality\u2019 in a biological context.\u00a0\u21a9</p> </li> <li> <p>It isn\u2019t \u2013 try Googling the testing of Challenger Space Shuttle o-rings, Volkswagen diesel emissions or Grenfell Tower cladding.\u00a0\u21a9</p> </li> <li> <p>If \\(d(a,b)\\) is a metric then \\(r_a^b= \\exp\\{-d(a,b)\\}\\) is a relevance. And the simplest metric is \\(\\big|b-a\\big|\\).\u00a0\u21a9\u21a9</p> </li> <li> <p>For instance, this is how the CMI adjusted its Mortality Projections Model to cope with COVID-affected mortality experience. I note that the debate then was essentially over the degree of relevance, not whether adjusting for relevance was wrong.\u00a0\u21a9</p> </li> <li> <p>In the same way that it would be unreasonable to assume that the mortality experience of all 800\u00a0million humans over age\u00a065 is 100% relevant \u2013 nominative determinism is not statistics.\u00a0\u21a9</p> </li> <li> <p>Model selection (e.g. using the AIC) will still be affected because it would be determined by lives affected, not liability impact, although I suspect that the difference in most cases is second order.\u00a0\u21a9</p> </li> <li> <p>Obvious because (a)\u00a0results should be independent of units, (b)\u00a0ratios are the simplest dimensionless means of comparison and (c)\u00a0measuring distance requires taking differences. Hence use logarithms.\u00a0\u21a9</p> </li> <li> <p>This follows from the properties of relevance, i.e.</p> <ul> <li>100% self-relevance, i.e. \\(r_{it}^{it}=1\\),</li> <li>symmetry, i.e. \\(r_{it}^{ju}= r_{ju}^{it}\\), and</li> <li>consistency, i.e. \\(r_{it}^{kv} \\ge r_{it}^{ju} \\cdot r_{ju}^{kv}\\).</li> </ul> <p>\u21a9</p> </li> <li> <p>Because this is a partition, we can use the same label for experience and valuation data.\u00a0\u21a9</p> </li> <li> <p>I haven\u2019t shown the maths, but this follows direct from the equations in Insights 15 and 16.\u00a0\u21a9</p> </li> <li> <p>The answer may well be that the nature of the problem means that the a priori belief is all experience data is 100% relevant to all individuals and the time scale is such that time-relevance does not need to be allowed for.\u00a0\u21a9</p> </li> <li> <p>This is closely related to the expert fallacy, which is:</p> <p>\u2018I can\u2019t define the process I will use but I am confident that I can get the right answer if I personally review every case.\u2019\u00a0\u21a9</p> </li> <li> <p>A lot of systems can\u2019t weight by anything.\u00a0\u21a9</p> </li> </ol>"},{"location":"topic/maths--stats/","title":"Maths &amp; stats","text":""},{"location":"topic/mortality--longevity/","title":"Mortality &amp; longevity","text":""},{"location":"topic/coding/","title":"Coding","text":""},{"location":"topic/presentation/","title":"Presentation","text":""},{"location":"page/2/","title":"Blog","text":""},{"location":"topic/maths--stats/page/2/","title":"Maths &amp; stats","text":""},{"location":"topic/mortality--longevity/page/2/","title":"Mortality &amp; longevity","text":""}]}